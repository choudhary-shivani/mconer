{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe9d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from dataloader import CoNLLReader\n",
    "from tqdm import tqdm\n",
    "from tutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8961a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del globals()['NERmodelbase']\n",
    "except:\n",
    "    pass\n",
    "from NERmodel3 import NERmodelbase3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be7f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bc8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    batch_ = list(zip(*batch))\n",
    "    tokens, masks, token_masks, gold_spans, tags, lstm_encoded = batch_[0], batch_[1], batch_[2], batch_[3], \\\n",
    "                                                                 batch_[4], batch_[5]\n",
    "    # print(tags)\n",
    "    max_len = np.max([len(token) for token in tokens])\n",
    "    # print(np.max([len(token) for token in tokens]), max_len)\n",
    "    token_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(tokenizer.pad_token_id)\n",
    "    tag_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(mconern['O'])\n",
    "    mask_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n",
    "    token_masks_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n",
    "    lstm_encoded_tensor = torch.zeros(size=(len(tokens), max_len, 256), dtype=torch.float)\n",
    "    # print(lstm_encoded.shape)\n",
    "    for i in range(len(tokens)):\n",
    "        tokens_ = tokens[i]\n",
    "        seq_len = len(tokens_)\n",
    "\n",
    "        token_tensor[i, :seq_len] = tokens_\n",
    "        tag_tensor[i, :seq_len] = tags[i]\n",
    "        mask_tensor[i, :seq_len] = masks[i]\n",
    "        token_masks_tensor[i, :seq_len] = token_masks[i]\n",
    "        lstm_encoded_tensor[i, 1:seq_len - 1, :] = lstm_encoded[i]\n",
    "\n",
    "    return token_tensor, tag_tensor, mask_tensor, token_masks_tensor, gold_spans, lstm_encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fdbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, opt=False):\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-4, weight_decay=0.03)\n",
    "    if opt:\n",
    "        scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEP)\n",
    "        return [optimizer], [scheduler]\n",
    "    return [optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7f43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 1\n",
    "BATCH_SIZE = 64\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52888663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnut_iob = {'B-CORP': 0, 'I-CORP': 1, 'B-CW': 2, 'I-CW': 3, 'B-GRP': 4, 'I-GRP': 5, 'B-LOC': 6, 'I-LOC': 7,\n",
    "#             'B-PER': 8, 'I-PER': 9, 'B-PROD': 10, 'I-PROD': 11, 'O': 12}\n",
    "mconern = indvidual(mconer_grouped, True)\n",
    "reveremap = invert(mconer_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f48032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if os.path.exists('train_load.pkl'):\n",
    "    with open('train_load.pkl', 'rb') as f:\n",
    "        ds = pickle.load(f)\n",
    "else:\n",
    "    print(\"reading from disk\")\n",
    "    ds = CoNLLReader(target_vocab=mconern, encoder_model=encoder_model, reversemap=reveremap, finegrained=fine)\n",
    "    ds.read_data(data=r'C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-train.conll')\n",
    "    with open('train_load.pkl', 'wb') as f:\n",
    "        pickle.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268eafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('valid_load.pkl'):\n",
    "    with open('valid_load.pkl', 'rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "else:\n",
    "    valid = CoNLLReader(target_vocab=mconern, encoder_model=encoder_model, reversemap=reveremap, finegrained=True)\n",
    "    valid.read_data(data=r'C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-dev.conll')\n",
    "    with open('valid_load.pkl', 'wb') as f:\n",
    "        pickle.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0a954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Rah12937\\PycharmProjects\\mconer\\NERmodel3.py:27: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.w_omega, -0.1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "model = NERmodelbase3(tag_to_id=mconern, device=device, encoder_model=encoder_model, dropout=0.3, use_lstm=True).to(\n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a362ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_batch, num_workers=0, shuffle=False)\n",
    "validloader = DataLoader(valid, batch_size=BATCH_SIZE, collate_fn=collate_batch, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14338915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of warm up step is 26\n"
     ]
    }
   ],
   "source": [
    "WARMUP_STEP = int(len(trainloader) * NUM_EPOCH * 0.1)\n",
    "print(f\"Number of warm up step is {WARMUP_STEP}\")\n",
    "optim, scheduler = get_optimizer(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02820976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "run_id = random.randint(1, 10000)\n",
    "run_name = f\"runid_{run_id}_EP_{NUM_EPOCH}_fine_xlm-b-birnnn-focal-loss-0_8-sep_lr-alpha-2-gama-4\"\n",
    "writer = SummaryWriter(run_name)\n",
    "step = 0\n",
    "running_loss = 0\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=run_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0758cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f140216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  37%|███████████████████████████████████████████████████████████▌                                                                                                    | 98/263 [01:07<01:52,  1.46batch/s]\n",
      "  0%|                                                                                                                                                                                   | 0/14 [00:00<?, ?batch/s]\u001b[A\n",
      "  7%|████████████▏                                                                                                                                                              | 1/14 [00:00<00:07,  1.77batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1259e+01, -1.0008e+04, -8.2031e+00,  ..., -8.7335e+00,\n",
      "         -1.0009e+04, -1.0547e+00],\n",
      "        [-1.0671e+01, -1.9902e+01, -1.0335e+01,  ..., -9.6462e+00,\n",
      "         -1.7437e+01, -1.0900e+00],\n",
      "        [-7.3997e+00, -1.6369e+01, -7.2292e+00,  ..., -6.5972e+00,\n",
      "         -1.5234e+01, -1.5071e+00],\n",
      "        ...,\n",
      "        [-3.7593e+01, -3.7997e+01, -3.7270e+01,  ..., -3.6562e+01,\n",
      "         -3.7791e+01, -2.7648e+01],\n",
      "        [-3.7563e+01, -4.6462e+01, -3.7224e+01,  ..., -3.6527e+01,\n",
      "         -4.5734e+01, -2.7679e+01],\n",
      "        [-3.9265e+01, -4.5919e+01, -3.8370e+01,  ..., -3.6927e+01,\n",
      "         -4.8371e+01, -2.6347e+01]], device='cuda:0') tensor([72, 50, 51,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.5233, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|████████████████████████▍                                                                                                                                                  | 2/14 [00:00<00:05,  2.16batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1323e+01, -1.0009e+04, -8.2771e+00,  ..., -8.8125e+00,\n",
      "         -1.0009e+04, -1.0540e+00],\n",
      "        [-1.0753e+01, -2.0031e+01, -1.0424e+01,  ..., -9.7350e+00,\n",
      "         -1.7653e+01, -1.0880e+00],\n",
      "        [-5.8942e+00, -1.5405e+01, -5.9268e+00,  ..., -5.4355e+00,\n",
      "         -1.4151e+01, -3.3925e+00],\n",
      "        ...,\n",
      "        [-7.2552e+01, -7.4089e+01, -7.2614e+01,  ..., -7.2125e+01,\n",
      "         -7.3353e+01, -7.1020e+01],\n",
      "        [-7.5641e+01, -7.7182e+01, -7.5714e+01,  ..., -7.5236e+01,\n",
      "         -7.6463e+01, -7.4142e+01],\n",
      "        [-8.5666e+01, -8.3933e+01, -8.4774e+01,  ..., -8.3315e+01,\n",
      "         -8.6942e+01, -7.2811e+01]], device='cuda:0') tensor([72, 40, 41,  ..., 41, 41, 72], device='cuda:0')\n",
      "tensor(4.2999, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|████████████████████████████████████▋                                                                                                                                      | 3/14 [00:01<00:04,  2.30batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1264e+01, -1.0008e+04, -8.2312e+00,  ..., -8.7586e+00,\n",
      "         -1.0009e+04, -1.0542e+00],\n",
      "        [-1.0848e+01, -2.0096e+01, -1.0533e+01,  ..., -9.8345e+00,\n",
      "         -1.7703e+01, -1.0864e+00],\n",
      "        [-1.1299e+01, -2.0165e+01, -1.0956e+01,  ..., -1.0245e+01,\n",
      "         -1.9230e+01, -1.1149e+00],\n",
      "        ...,\n",
      "        [-3.6037e+01, -4.5310e+01, -3.5716e+01,  ..., -3.4973e+01,\n",
      "         -4.4303e+01, -2.5839e+01],\n",
      "        [-3.5902e+01, -4.5195e+01, -3.5557e+01,  ..., -3.4868e+01,\n",
      "         -4.4263e+01, -2.5869e+01],\n",
      "        [-3.7506e+01, -4.4297e+01, -3.6604e+01,  ..., -3.5163e+01,\n",
      "         -4.6728e+01, -2.4537e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.7286, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████████████████████████████▊                                                                                                                          | 4/14 [00:01<00:04,  2.32batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1229e+01, -1.0008e+04, -8.1836e+00,  ..., -8.7138e+00,\n",
      "         -1.0009e+04, -1.0547e+00],\n",
      "        [-1.0894e+01, -2.0125e+01, -1.0557e+01,  ..., -9.8767e+00,\n",
      "         -1.7664e+01, -1.0862e+00],\n",
      "        [-1.1197e+01, -2.0070e+01, -1.0879e+01,  ..., -1.0138e+01,\n",
      "         -1.9131e+01, -1.1161e+00],\n",
      "        ...,\n",
      "        [-3.1608e+01, -4.0683e+01, -3.1284e+01,  ..., -3.0582e+01,\n",
      "         -3.9767e+01, -2.1655e+01],\n",
      "        [-3.1622e+01, -4.0681e+01, -3.1289e+01,  ..., -3.0592e+01,\n",
      "         -3.9799e+01, -2.1685e+01],\n",
      "        [-3.3257e+01, -3.9976e+01, -3.2373e+01,  ..., -3.0931e+01,\n",
      "         -4.2420e+01, -2.0354e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.4289, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████████████████████████████████████                                                                                                              | 5/14 [00:02<00:03,  2.33batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1220e+01, -1.0008e+04, -8.1855e+00,  ..., -8.7218e+00,\n",
      "         -1.0009e+04, -1.0546e+00],\n",
      "        [-1.0875e+01, -2.0081e+01, -1.0550e+01,  ..., -9.8641e+00,\n",
      "         -1.7685e+01, -1.0864e+00],\n",
      "        [-1.1160e+01, -2.0089e+01, -1.0815e+01,  ..., -1.0147e+01,\n",
      "         -1.9165e+01, -1.1154e+00],\n",
      "        ...,\n",
      "        [-2.0757e+01, -2.9965e+01, -2.0425e+01,  ..., -1.9771e+01,\n",
      "         -2.9081e+01, -1.1011e+01],\n",
      "        [-2.0780e+01, -2.9655e+01, -2.0440e+01,  ..., -1.9783e+01,\n",
      "         -2.8816e+01, -1.1043e+01],\n",
      "        [-2.2624e+01, -2.9133e+01, -2.1726e+01,  ..., -2.0304e+01,\n",
      "         -3.1639e+01, -9.7114e+00]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.4356, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|█████████████████████████████████████████████████████████████████████████▎                                                                                                 | 6/14 [00:02<00:03,  2.34batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1101e+01, -1.0008e+04, -8.0828e+00,  ..., -8.6081e+00,\n",
      "         -1.0009e+04, -1.0556e+00],\n",
      "        [-1.0894e+01, -2.0006e+01, -1.0579e+01,  ..., -9.8935e+00,\n",
      "         -1.7599e+01, -1.0866e+00],\n",
      "        [-1.1314e+01, -2.0230e+01, -1.0994e+01,  ..., -1.0265e+01,\n",
      "         -1.9323e+01, -1.1147e+00],\n",
      "        ...,\n",
      "        [-3.4578e+01, -3.5057e+01, -3.4232e+01,  ..., -3.3539e+01,\n",
      "         -3.4793e+01, -2.4538e+01],\n",
      "        [-3.4566e+01, -4.3610e+01, -3.4208e+01,  ..., -3.3527e+01,\n",
      "         -4.2788e+01, -2.4567e+01],\n",
      "        [-3.6205e+01, -4.2960e+01, -3.5304e+01,  ..., -3.3869e+01,\n",
      "         -4.5396e+01, -2.3236e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.5973, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 7/14 [00:03<00:02,  2.36batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1346e+01, -1.0009e+04, -8.3000e+00,  ..., -8.8366e+00,\n",
      "         -1.0009e+04, -1.0537e+00],\n",
      "        [-1.0810e+01, -2.0112e+01, -1.0485e+01,  ..., -9.7898e+00,\n",
      "         -1.7738e+01, -1.0868e+00],\n",
      "        [-1.1366e+01, -2.0156e+01, -1.1023e+01,  ..., -1.0301e+01,\n",
      "         -1.9256e+01, -1.1146e+00],\n",
      "        ...,\n",
      "        [-1.2324e+01, -2.1612e+01, -1.1971e+01,  ..., -1.1273e+01,\n",
      "         -2.0713e+01, -2.0904e+00],\n",
      "        [-1.2312e+01, -2.1581e+01, -1.1946e+01,  ..., -1.1259e+01,\n",
      "         -2.0690e+01, -2.1186e+00],\n",
      "        [-1.3826e+01, -2.0735e+01, -1.2910e+01,  ..., -1.1487e+01,\n",
      "         -2.3209e+01, -7.8652e-01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(5.0346, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 8/14 [00:03<00:02,  2.39batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1382e+01, -1.0009e+04, -8.3305e+00,  ..., -8.8572e+00,\n",
      "         -1.0009e+04, -1.0536e+00],\n",
      "        [-1.0789e+01, -2.0108e+01, -1.0463e+01,  ..., -9.7562e+00,\n",
      "         -1.7737e+01, -1.0873e+00],\n",
      "        [-1.1361e+01, -2.0125e+01, -1.1019e+01,  ..., -1.0304e+01,\n",
      "         -1.9220e+01, -1.1151e+00],\n",
      "        ...,\n",
      "        [-1.8422e+01, -2.7708e+01, -1.8075e+01,  ..., -1.7372e+01,\n",
      "         -2.6799e+01, -8.2602e+00],\n",
      "        [-1.8420e+01, -2.7643e+01, -1.8063e+01,  ..., -1.7368e+01,\n",
      "         -2.6749e+01, -8.2887e+00],\n",
      "        [-1.9975e+01, -2.6832e+01, -1.9075e+01,  ..., -1.7636e+01,\n",
      "         -2.9286e+01, -6.9568e+00]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(4.0720, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 9/14 [00:03<00:02,  2.41batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1310e+01, -1.0009e+04, -8.2670e+00,  ..., -8.8036e+00,\n",
      "         -1.0009e+04, -1.0539e+00],\n",
      "        [-1.0844e+01, -2.0115e+01, -1.0525e+01,  ..., -9.8302e+00,\n",
      "         -1.7753e+01, -1.0864e+00],\n",
      "        [-1.0974e+01, -1.9876e+01, -1.0632e+01,  ..., -9.9601e+00,\n",
      "         -1.8969e+01, -1.1168e+00],\n",
      "        ...,\n",
      "        [-2.3926e+01, -2.6231e+01, -2.3593e+01,  ..., -2.2881e+01,\n",
      "         -2.5793e+01, -1.3858e+01],\n",
      "        [-2.3909e+01, -3.3060e+01, -2.3564e+01,  ..., -2.2863e+01,\n",
      "         -3.2144e+01, -1.3887e+01],\n",
      "        [-2.5583e+01, -3.2333e+01, -2.4683e+01,  ..., -2.3230e+01,\n",
      "         -3.4780e+01, -1.2555e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.5446, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 10/14 [00:04<00:01,  2.50batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1290e+01, -1.0009e+04, -8.2444e+00,  ..., -8.7803e+00,\n",
      "         -1.0009e+04, -1.0541e+00],\n",
      "        [-1.0820e+01, -2.0087e+01, -1.0492e+01,  ..., -9.8001e+00,\n",
      "         -1.7670e+01, -1.0870e+00],\n",
      "        [-5.7602e+00, -1.5429e+01, -5.7955e+00,  ..., -5.3544e+00,\n",
      "         -1.4131e+01, -3.9795e+00],\n",
      "        ...,\n",
      "        [-2.3793e+01, -3.2781e+01, -2.3460e+01,  ..., -2.2785e+01,\n",
      "         -3.1908e+01, -1.4053e+01],\n",
      "        [-2.3822e+01, -3.2697e+01, -2.3476e+01,  ..., -2.2807e+01,\n",
      "         -3.1836e+01, -1.4085e+01],\n",
      "        [-2.5609e+01, -3.2128e+01, -2.4712e+01,  ..., -2.3284e+01,\n",
      "         -3.4605e+01, -1.2754e+01]], device='cuda:0') tensor([72,  4,  5,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(4.0704, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 11/14 [00:04<00:01,  2.59batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1158e+01, -1.0008e+04, -8.1217e+00,  ..., -8.6648e+00,\n",
      "         -1.0009e+04, -1.0551e+00],\n",
      "        [-1.0934e+01, -2.0094e+01, -1.0598e+01,  ..., -9.9248e+00,\n",
      "         -1.7687e+01, -1.0859e+00],\n",
      "        [-1.1077e+01, -2.0059e+01, -1.0741e+01,  ..., -1.0042e+01,\n",
      "         -1.9167e+01, -1.1155e+00],\n",
      "        ...,\n",
      "        [-3.3734e+01, -3.5846e+01, -3.3414e+01,  ..., -3.2736e+01,\n",
      "         -3.5458e+01, -2.3981e+01],\n",
      "        [-3.3677e+01, -4.2576e+01, -3.3347e+01,  ..., -3.2683e+01,\n",
      "         -4.1708e+01, -2.4014e+01],\n",
      "        [-3.5666e+01, -4.2067e+01, -3.4761e+01,  ..., -3.3324e+01,\n",
      "         -4.4549e+01, -2.2682e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(4.8810, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 12/14 [00:05<00:00,  2.50batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1280e+01, -1.0008e+04, -8.2413e+00,  ..., -8.7715e+00,\n",
      "         -1.0009e+04, -1.0542e+00],\n",
      "        [-1.0880e+01, -2.0119e+01, -1.0556e+01,  ..., -9.8593e+00,\n",
      "         -1.7730e+01, -1.0862e+00],\n",
      "        [-1.1308e+01, -2.0191e+01, -1.0978e+01,  ..., -1.0261e+01,\n",
      "         -1.9247e+01, -1.1145e+00],\n",
      "        ...,\n",
      "        [-3.5495e+01, -3.8826e+01, -3.5159e+01,  ..., -3.4453e+01,\n",
      "         -3.8330e+01, -2.5433e+01],\n",
      "        [-3.5463e+01, -4.4608e+01, -3.5123e+01,  ..., -3.4427e+01,\n",
      "         -4.3722e+01, -2.5462e+01],\n",
      "        [-3.7184e+01, -4.3901e+01, -3.6273e+01,  ..., -3.4833e+01,\n",
      "         -4.6381e+01, -2.4130e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.7363, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 13/14 [00:05<00:00,  2.48batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1326e+01, -1.0009e+04, -8.2748e+00,  ..., -8.8169e+00,\n",
      "         -1.0009e+04, -1.0539e+00],\n",
      "        [-1.0809e+01, -2.0093e+01, -1.0483e+01,  ..., -9.7920e+00,\n",
      "         -1.7717e+01, -1.0869e+00],\n",
      "        [-1.1249e+01, -2.0068e+01, -1.0887e+01,  ..., -1.0199e+01,\n",
      "         -1.9162e+01, -1.1155e+00],\n",
      "        ...,\n",
      "        [-6.8807e+01, -6.9274e+01, -6.8478e+01,  ..., -6.7781e+01,\n",
      "         -6.9030e+01, -5.8868e+01],\n",
      "        [-6.8780e+01, -7.7734e+01, -6.8441e+01,  ..., -6.7757e+01,\n",
      "         -7.6950e+01, -5.8898e+01],\n",
      "        [-7.0539e+01, -7.7169e+01, -6.9637e+01,  ..., -6.8184e+01,\n",
      "         -7.9612e+01, -5.7567e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.4025, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.44batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1334e+01, -1.0009e+04, -8.2843e+00,  ..., -8.8112e+00,\n",
      "         -1.0009e+04, -1.0539e+00],\n",
      "        [-1.0779e+01, -2.0064e+01, -1.0447e+01,  ..., -9.7448e+00,\n",
      "         -1.7668e+01, -1.0876e+00],\n",
      "        [-1.1316e+01, -2.0084e+01, -1.0951e+01,  ..., -1.0251e+01,\n",
      "         -1.9167e+01, -1.1157e+00],\n",
      "        ...,\n",
      "        [-1.4470e+01, -2.3612e+01, -1.4133e+01,  ..., -1.3454e+01,\n",
      "         -2.2721e+01, -4.5135e+00],\n",
      "        [-1.4499e+01, -2.3553e+01, -1.4152e+01,  ..., -1.3482e+01,\n",
      "         -2.2680e+01, -4.5431e+00],\n",
      "        [-1.6257e+01, -2.2953e+01, -1.5351e+01,  ..., -1.3927e+01,\n",
      "         -2.5453e+01, -3.2107e+00]], device='cuda:0') tensor([72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 28, 29, 29,\n",
      "        29, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 54,\n",
      "        55, 55, 55, 72, 72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 44, 45, 45, 45, 45, 72, 72, 72,\n",
      "        28, 29, 29, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 36, 37, 37, 12, 13, 72,\n",
      "        28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 68, 69, 69, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 69, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69,\n",
      "        69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 70, 71,\n",
      "        72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7, 72, 40, 41, 41, 41,\n",
      "        41, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72,  6,  7,  7, 72, 50, 51, 51, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72,  6,  7, 72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7,  7,  7,  7, 72, 40, 41, 41, 41, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7,  7,  7,  7,  7, 72,  6,  7,  7,  7,  7,  7,  7, 72,\n",
      "        72, 72, 72, 40, 41, 41, 41, 41, 72, 40, 41, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7, 72, 72,\n",
      "         6,  7,  7,  7,  7, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7,  7,\n",
      "         7,  7, 72, 40, 41, 41, 41, 41, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "         6,  7,  7, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 24, 25, 72, 72, 72, 72, 72, 72, 40, 41, 41, 41, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 24, 25, 25, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 24, 25, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 32, 33, 36, 72, 24, 25, 72, 72, 72, 72, 72, 24, 25, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 24, 25, 25, 25, 25, 25, 25, 25, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 24, 25, 25, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 24, 25, 25, 25, 25, 25, 25, 16, 17, 17, 17, 17, 17, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 22, 23, 23, 23, 22, 23, 23, 24, 25,\n",
      "        25, 25, 22, 23, 23, 23, 72, 22, 23, 23, 72, 72, 72, 72, 72, 24, 25, 25,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72], device='cuda:0')\n",
      "tensor(3.7909, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0806\n",
      "Validation loss decreased (inf --> 1.080600).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 198/263 [02:24<00:46,  1.40batch/s]\n",
      "  0%|                                                                                                                                                                                   | 0/14 [00:00<?, ?batch/s]\u001b[A\n",
      "  7%|████████████▏                                                                                                                                                              | 1/14 [00:00<00:06,  1.87batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.0412e+00, -1.0007e+04, -8.0746e+00,  ..., -8.4767e+00,\n",
      "         -1.0009e+04, -1.0554e+00],\n",
      "        [-7.1213e+00, -1.4684e+01, -8.7745e+00,  ..., -7.9318e+00,\n",
      "         -1.5342e+01, -1.1703e+00],\n",
      "        [-5.2888e+00, -1.1870e+01, -5.8623e+00,  ..., -5.5004e+00,\n",
      "         -1.2465e+01, -3.2819e+00],\n",
      "        ...,\n",
      "        [-3.4838e+01, -3.5887e+01, -3.6531e+01,  ..., -3.5741e+01,\n",
      "         -3.7203e+01, -2.7541e+01],\n",
      "        [-3.4812e+01, -4.1937e+01, -3.6490e+01,  ..., -3.5710e+01,\n",
      "         -4.4002e+01, -2.7583e+01],\n",
      "        [-3.6669e+01, -4.1406e+01, -3.7766e+01,  ..., -3.6238e+01,\n",
      "         -4.6850e+01, -2.6262e+01]], device='cuda:0') tensor([72, 50, 51,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.5537, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|████████████████████████▍                                                                                                                                                  | 2/14 [00:00<00:05,  2.28batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.3458e+00, -1.0006e+04, -7.2473e+00,  ..., -7.6691e+00,\n",
      "         -1.0008e+04, -1.0754e+00],\n",
      "        [-7.4113e+00, -1.4463e+01, -9.0763e+00,  ..., -8.2135e+00,\n",
      "         -1.4846e+01, -1.1705e+00],\n",
      "        [-5.2930e+00, -1.2189e+01, -5.8026e+00,  ..., -5.4610e+00,\n",
      "         -1.2719e+01, -3.5697e+00],\n",
      "        ...,\n",
      "        [-5.2123e+01, -5.5012e+01, -5.2773e+01,  ..., -5.2360e+01,\n",
      "         -5.5104e+01, -4.9773e+01],\n",
      "        [-5.3936e+01, -5.6849e+01, -5.4598e+01,  ..., -5.4177e+01,\n",
      "         -5.6960e+01, -5.1545e+01],\n",
      "        [-5.7972e+01, -5.8400e+01, -5.8412e+01,  ..., -5.7121e+01,\n",
      "         -6.2318e+01, -5.0666e+01]], device='cuda:0') tensor([72, 40, 41,  ..., 41, 41, 72], device='cuda:0')\n",
      "tensor(3.5508, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|████████████████████████████████████▋                                                                                                                                      | 3/14 [00:01<00:04,  2.38batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.4441e+00, -1.0006e+04, -7.3290e+00,  ..., -7.8052e+00,\n",
      "         -1.0008e+04, -1.0675e+00],\n",
      "        [-8.4767e+00, -1.5635e+01, -1.0239e+01,  ..., -9.4508e+00,\n",
      "         -1.6215e+01, -1.1075e+00],\n",
      "        [-5.7020e+00, -1.3475e+01, -6.6919e+00,  ..., -6.1455e+00,\n",
      "         -1.4641e+01, -1.7739e+00],\n",
      "        ...,\n",
      "        [-1.4527e+01, -1.8940e+01, -1.5607e+01,  ..., -1.4988e+01,\n",
      "         -1.9651e+01, -1.0338e+01],\n",
      "        [-1.5556e+01, -1.9946e+01, -1.6767e+01,  ..., -1.6138e+01,\n",
      "         -2.0934e+01, -1.0620e+01],\n",
      "        [-1.9147e+01, -2.1746e+01, -2.0159e+01,  ..., -1.8621e+01,\n",
      "         -2.6654e+01, -9.3212e+00]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.1312, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████████████████████████████▊                                                                                                                          | 4/14 [00:01<00:04,  2.34batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.6438e+00, -1.0007e+04, -7.5777e+00,  ..., -8.0317e+00,\n",
      "         -1.0008e+04, -1.0623e+00],\n",
      "        [-8.3641e+00, -1.5777e+01, -1.0101e+01,  ..., -9.3276e+00,\n",
      "         -1.6319e+01, -1.1054e+00],\n",
      "        [-5.7258e+00, -1.3341e+01, -6.7520e+00,  ..., -6.2023e+00,\n",
      "         -1.4555e+01, -1.7144e+00],\n",
      "        ...,\n",
      "        [-3.1268e+01, -3.3815e+01, -3.2951e+01,  ..., -3.2170e+01,\n",
      "         -3.5292e+01, -2.3985e+01],\n",
      "        [-3.1244e+01, -3.8362e+01, -3.2910e+01,  ..., -3.2140e+01,\n",
      "         -4.0423e+01, -2.4027e+01],\n",
      "        [-3.3340e+01, -3.8027e+01, -3.4492e+01,  ..., -3.2950e+01,\n",
      "         -4.3542e+01, -2.2703e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.8394, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████████████████████████████████████                                                                                                              | 5/14 [00:02<00:03,  2.36batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.0604e+00, -1.0007e+04, -8.0763e+00,  ..., -8.4965e+00,\n",
      "         -1.0009e+04, -1.0543e+00],\n",
      "        [-8.4304e+00, -1.5995e+01, -1.0246e+01,  ..., -9.4036e+00,\n",
      "         -1.6835e+01, -1.0947e+00],\n",
      "        [-9.0303e+00, -1.6082e+01, -1.0876e+01,  ..., -1.0065e+01,\n",
      "         -1.8383e+01, -1.1244e+00],\n",
      "        ...,\n",
      "        [-1.8118e+01, -2.3834e+01, -1.9966e+01,  ..., -1.9126e+01,\n",
      "         -2.5990e+01, -1.0106e+01],\n",
      "        [-1.8136e+01, -2.5813e+01, -1.9978e+01,  ..., -1.9141e+01,\n",
      "         -2.8227e+01, -1.0134e+01],\n",
      "        [-1.9636e+01, -2.5047e+01, -2.0832e+01,  ..., -1.9256e+01,\n",
      "         -3.0736e+01, -8.8062e+00]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.3262, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|█████████████████████████████████████████████████████████████████████████▎                                                                                                 | 6/14 [00:02<00:03,  2.40batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8646e+00, -1.0007e+04, -7.8429e+00,  ..., -8.2793e+00,\n",
      "         -1.0008e+04, -1.0574e+00],\n",
      "        [-8.4623e+00, -1.5957e+01, -1.0254e+01,  ..., -9.4432e+00,\n",
      "         -1.6669e+01, -1.0973e+00],\n",
      "        [-7.9285e+00, -1.5224e+01, -9.5578e+00,  ..., -8.7931e+00,\n",
      "         -1.7191e+01, -1.1550e+00],\n",
      "        ...,\n",
      "        [-2.9602e+01, -3.0643e+01, -3.1351e+01,  ..., -3.0545e+01,\n",
      "         -3.2067e+01, -2.2028e+01],\n",
      "        [-2.9612e+01, -3.6942e+01, -3.1357e+01,  ..., -3.0558e+01,\n",
      "         -3.9149e+01, -2.2064e+01],\n",
      "        [-3.1131e+01, -3.6175e+01, -3.2225e+01,  ..., -3.0699e+01,\n",
      "         -4.1682e+01, -2.0743e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.2834, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 7/14 [00:02<00:02,  2.47batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.9655e+00, -1.0007e+04, -7.9603e+00,  ..., -8.3910e+00,\n",
      "         -1.0008e+04, -1.0557e+00],\n",
      "        [-8.5803e+00, -1.6092e+01, -1.0409e+01,  ..., -9.5728e+00,\n",
      "         -1.6902e+01, -1.0928e+00],\n",
      "        [-5.8425e+00, -1.3695e+01, -6.8924e+00,  ..., -6.3208e+00,\n",
      "         -1.4936e+01, -1.6173e+00],\n",
      "        ...,\n",
      "        [-1.5532e+01, -2.1309e+01, -1.7198e+01,  ..., -1.6420e+01,\n",
      "         -2.3203e+01, -8.3205e+00],\n",
      "        [-1.5591e+01, -2.2617e+01, -1.7260e+01,  ..., -1.6487e+01,\n",
      "         -2.4675e+01, -8.3626e+00],\n",
      "        [-1.7791e+01, -2.2447e+01, -1.8963e+01,  ..., -1.7399e+01,\n",
      "         -2.8002e+01, -7.0358e+00]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.3196, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 8/14 [00:03<00:02,  2.50batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.9665e+00, -1.0007e+04, -7.9761e+00,  ..., -8.3891e+00,\n",
      "         -1.0008e+04, -1.0558e+00],\n",
      "        [-8.3330e+00, -1.5856e+01, -1.0136e+01,  ..., -9.2865e+00,\n",
      "         -1.6623e+01, -1.0991e+00],\n",
      "        [-5.7090e+00, -1.3304e+01, -6.7337e+00,  ..., -6.1863e+00,\n",
      "         -1.4495e+01, -1.7190e+00],\n",
      "        ...,\n",
      "        [-2.8370e+01, -3.1837e+01, -2.9900e+01,  ..., -2.9171e+01,\n",
      "         -3.3209e+01, -2.1782e+01],\n",
      "        [-2.8385e+01, -3.4924e+01, -2.9898e+01,  ..., -2.9187e+01,\n",
      "         -3.6689e+01, -2.1850e+01],\n",
      "        [-3.0616e+01, -3.4727e+01, -3.1638e+01,  ..., -3.0137e+01,\n",
      "         -3.9970e+01, -2.0538e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.6540, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 9/14 [00:03<00:01,  2.52batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8062e+00, -1.0007e+04, -7.7700e+00,  ..., -8.2025e+00,\n",
      "         -1.0008e+04, -1.0587e+00],\n",
      "        [-8.5947e+00, -1.6001e+01, -1.0413e+01,  ..., -9.5799e+00,\n",
      "         -1.6736e+01, -1.0956e+00],\n",
      "        [-8.4539e+00, -1.5806e+01, -1.0170e+01,  ..., -9.3744e+00,\n",
      "         -1.7919e+01, -1.1358e+00],\n",
      "        ...,\n",
      "        [-2.6425e+01, -2.9233e+01, -2.8002e+01,  ..., -2.7257e+01,\n",
      "         -3.0563e+01, -1.9650e+01],\n",
      "        [-2.6438e+01, -3.3150e+01, -2.8002e+01,  ..., -2.7270e+01,\n",
      "         -3.4982e+01, -1.9710e+01],\n",
      "        [-2.8363e+01, -3.2705e+01, -2.9363e+01,  ..., -2.7876e+01,\n",
      "         -3.7929e+01, -1.8402e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.4598, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 10/14 [00:04<00:01,  2.54batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.1491e+00, -1.0007e+04, -8.2161e+00,  ..., -8.5915e+00,\n",
      "         -1.0009e+04, -1.0535e+00],\n",
      "        [-7.5556e+00, -1.5144e+01, -9.2968e+00,  ..., -8.4127e+00,\n",
      "         -1.5938e+01, -1.1304e+00],\n",
      "        [-5.2402e+00, -1.2287e+01, -5.8504e+00,  ..., -5.4776e+00,\n",
      "         -1.2945e+01, -3.1474e+00],\n",
      "        ...,\n",
      "        [-2.1365e+01, -2.7835e+01, -2.3184e+01,  ..., -2.2359e+01,\n",
      "         -3.0075e+01, -1.3448e+01],\n",
      "        [-2.1402e+01, -2.9004e+01, -2.3217e+01,  ..., -2.2398e+01,\n",
      "         -3.1389e+01, -1.3477e+01],\n",
      "        [-2.2881e+01, -2.8236e+01, -2.4048e+01,  ..., -2.2491e+01,\n",
      "         -3.3897e+01, -1.2150e+01]], device='cuda:0') tensor([72,  4,  5,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.0232, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 11/14 [00:04<00:01,  2.57batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8636e+00, -1.0007e+04, -7.8474e+00,  ..., -8.2732e+00,\n",
      "         -1.0008e+04, -1.0574e+00],\n",
      "        [-8.5764e+00, -1.6070e+01, -1.0385e+01,  ..., -9.5674e+00,\n",
      "         -1.6793e+01, -1.0946e+00],\n",
      "        [-6.5214e+00, -1.4187e+01, -7.8063e+00,  ..., -7.1460e+00,\n",
      "         -1.5725e+01, -1.3160e+00],\n",
      "        ...,\n",
      "        [-3.2596e+01, -3.4834e+01, -3.4323e+01,  ..., -3.3526e+01,\n",
      "         -3.6348e+01, -2.5112e+01],\n",
      "        [-3.2590e+01, -3.9847e+01, -3.4302e+01,  ..., -3.3520e+01,\n",
      "         -4.2018e+01, -2.5149e+01],\n",
      "        [-3.4273e+01, -3.9190e+01, -3.5370e+01,  ..., -3.3846e+01,\n",
      "         -4.4708e+01, -2.3827e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(3.7690, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 12/14 [00:04<00:00,  2.51batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.6986e+00, -1.0007e+04, -7.6419e+00,  ..., -8.0939e+00,\n",
      "         -1.0008e+04, -1.0611e+00],\n",
      "        [-8.4885e+00, -1.5836e+01, -1.0271e+01,  ..., -9.4643e+00,\n",
      "         -1.6509e+01, -1.1007e+00],\n",
      "        [-6.4125e+00, -1.3993e+01, -7.6568e+00,  ..., -7.0314e+00,\n",
      "         -1.5493e+01, -1.3477e+00],\n",
      "        ...,\n",
      "        [-4.5044e+01, -4.7675e+01, -4.6495e+01,  ..., -4.5797e+01,\n",
      "         -4.8862e+01, -3.8774e+01],\n",
      "        [-4.5178e+01, -5.1423e+01, -4.6642e+01,  ..., -4.5945e+01,\n",
      "         -5.3095e+01, -3.8855e+01],\n",
      "        [-4.7485e+01, -5.1406e+01, -4.8475e+01,  ..., -4.6991e+01,\n",
      "         -5.6578e+01, -3.7548e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.9710, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 13/14 [00:05<00:00,  2.49batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.6756e+00, -1.0007e+04, -7.6055e+00,  ..., -8.0581e+00,\n",
      "         -1.0008e+04, -1.0618e+00],\n",
      "        [-8.5299e+00, -1.5823e+01, -1.0321e+01,  ..., -9.4964e+00,\n",
      "         -1.6507e+01, -1.1006e+00],\n",
      "        [-6.4063e+00, -1.4054e+01, -7.6257e+00,  ..., -6.9917e+00,\n",
      "         -1.5492e+01, -1.3632e+00],\n",
      "        ...,\n",
      "        [-7.2599e+01, -7.4151e+01, -7.4107e+01,  ..., -7.3388e+01,\n",
      "         -7.5276e+01, -6.6088e+01],\n",
      "        [-7.2813e+01, -7.9255e+01, -7.4352e+01,  ..., -7.3631e+01,\n",
      "         -8.1038e+01, -6.6150e+01],\n",
      "        [-7.5218e+01, -7.9412e+01, -7.6311e+01,  ..., -7.4792e+01,\n",
      "         -8.4747e+01, -6.4831e+01]], device='cuda:0') tensor([72, 72, 72,  ..., 72, 72, 72], device='cuda:0')\n",
      "tensor(2.7744, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.50batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.7615e+00, -1.0007e+04, -7.7149e+00,  ..., -8.1475e+00,\n",
      "         -1.0008e+04, -1.0599e+00],\n",
      "        [-8.4746e+00, -1.5815e+01, -1.0276e+01,  ..., -9.4345e+00,\n",
      "         -1.6516e+01, -1.1000e+00],\n",
      "        [-6.4489e+00, -1.4010e+01, -7.6745e+00,  ..., -7.0442e+00,\n",
      "         -1.5481e+01, -1.3455e+00],\n",
      "        ...,\n",
      "        [-1.8276e+01, -2.3710e+01, -1.9880e+01,  ..., -1.9127e+01,\n",
      "         -2.5467e+01, -1.1359e+01],\n",
      "        [-1.8243e+01, -2.5060e+01, -1.9824e+01,  ..., -1.9087e+01,\n",
      "         -2.6952e+01, -1.1414e+01],\n",
      "        [-2.0315e+01, -2.4681e+01, -2.1363e+01,  ..., -1.9858e+01,\n",
      "         -3.0016e+01, -1.0098e+01]], device='cuda:0') tensor([72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 28, 29, 29,\n",
      "        29, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 54,\n",
      "        55, 55, 55, 72, 72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 44, 45, 45, 45, 45, 72, 72, 72,\n",
      "        28, 29, 29, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        28, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 36, 37, 37, 12, 13, 72,\n",
      "        28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 28, 29, 29, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 68, 69, 69, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 69, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69,\n",
      "        69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 68, 69, 69, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 70, 71,\n",
      "        72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 68, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7, 72, 40, 41, 41, 41,\n",
      "        41, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72,  6,  7,  7, 72, 50, 51, 51, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72,  6,  7, 72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7,  7,  7,  7, 72, 40, 41, 41, 41, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7,  7,  7,  7,  7, 72,  6,  7,  7,  7,  7,  7,  7, 72,\n",
      "        72, 72, 72, 40, 41, 41, 41, 41, 72, 40, 41, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72,  6,  7,  7, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7, 72, 72,\n",
      "         6,  7,  7,  7,  7, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7,  7,\n",
      "         7,  7, 72, 40, 41, 41, 41, 41, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "         6,  7,  7, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 24, 25, 72, 72, 72, 72, 72, 72, 40, 41, 41, 41, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 24, 25, 25, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 24, 25, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 32, 33, 36, 72, 24, 25, 72, 72, 72, 72, 72, 24, 25, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 24, 25, 25, 25, 25, 25, 25, 25, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 24, 25, 25, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 24, 25, 25, 25, 25, 25, 25, 16, 17, 17, 17, 17, 17, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 22, 23, 23, 23, 22, 23, 23, 24, 25,\n",
      "        25, 25, 22, 23, 23, 23, 72, 22, 23, 23, 72, 72, 72, 72, 72, 24, 25, 25,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72], device='cuda:0')\n",
      "tensor(3.0905, device='cuda:0')\n",
      "1.0276\n",
      "Validation loss decreased (1.080600 --> 1.027600).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 263/263 [03:18<00:00,  1.32batch/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eval_step = 100\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    val_track = []\n",
    "    with tqdm(trainloader, unit='batch') as tepoch:\n",
    "        # model.train()\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        for i, data in enumerate(tepoch):\n",
    "            optim[0].zero_grad()\n",
    "            outputs, focal_loss, all_prob, token_scores, mask, metadata = model(data)\n",
    "            loss = 0.2 * outputs['loss'] + 0.8 * focal_loss\n",
    "            running_loss += loss\n",
    "            loss.backward()\n",
    "            optim[0].step()\n",
    "            scheduler[0].step()\n",
    "            # if i % 10 == 0:  # print every 2000 mini-batches\n",
    "            model.spanf1.reset()\n",
    "            # writer.add_scalar('lr', scheduler[0].get_last_lr()[0], step)\n",
    "            # run validation\n",
    "            step += 1\n",
    "            if (step + 1) % eval_step == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    with tqdm(validloader, unit='batch') as tepoch:\n",
    "                        val_loss = 0\n",
    "                        for i, data in enumerate(tepoch):\n",
    "                            outputs, focal_loss, all_prob, token_scores, mask, metadata = model(data, mode='predict')\n",
    "                            val_loss += 0.2 * outputs['loss'] + 0.8 * focal_loss\n",
    "                model.train()\n",
    "                writer.add_scalars(\"Loss\",\n",
    "                                   {\n",
    "                                       \"Train Loss\": round(running_loss.detach().cpu().numpy().ravel()[0] / 20, 4),\n",
    "                                       \"Valid Loss\": round(val_loss.detach().cpu().numpy().ravel()[0] / 20, 4),\n",
    "                                   }\n",
    "                                   , step)\n",
    "                writer.add_scalars(\"Metrics\", outputs['results'], step)\n",
    "                # print(outputs['results'])\n",
    "                # writer.add_scalar(\"Loss/Test\", round(val_loss.numpy()[0] / len(validloader), 4), step)\n",
    "                # writer.add_scalar(\"Loss/Valid\",\n",
    "                #                   round(val_loss.detach().cpu().numpy().ravel()[0] / 20, 4), step)\n",
    "                val_track.append(round(val_loss.detach().cpu().numpy().ravel()[0] / eval_step, 4))\n",
    "                running_loss = 0\n",
    "\n",
    "                early_stopping(round(val_loss.detach().cpu().numpy().ravel()[0] / eval_step, 4), model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Stopping early\")\n",
    "                    break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e250e9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 238])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(all_prob, dim=1).argmax(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ec371c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7268907563025211"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.cat(all_prob, dim=1), dim=-1).squeeze(0)\n",
    "torch.masked_select(metadata, mask)\n",
    "# torch.nn.CrossEntropyLoss()(torch.cat(all_prob, dim=1).squeeze(0),\n",
    "#                          torch.masked_select(metadata, mask))\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(torch.cat(all_prob, dim=1).argmax(-1).squeeze(0).cpu(),\n",
    "        torch.masked_select(metadata, mask).cpu(), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1a1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8132, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kornia.losses import FocalLoss\n",
    "fl = FocalLoss(alpha=2, gamma=5, reduction='mean')\n",
    "fl(token_scores.permute(0, 2, 1), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2091b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7399, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl(torch.cat(all_prob, dim=1).squeeze(0),\n",
    "    torch.masked_select(metadata, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb53902e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 34, 73])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.unsqueeze(-1).expand(token_scores.size()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9250e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17374])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(token_scores, mask.unsqueeze(-1).expand(token_scores.size())).ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f943b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(metadata, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e40f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 73])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prob[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbcb88c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 73)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf.transitions.cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a3cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([([4, 56, 10, 24, 16, 12, 0, 72, 28, 64], 24.494735717773438),\n",
       "  ([4, 5, 16, 20, 22, 62, 56, 28, 18, 20], 23.203876495361328)],\n",
       " [tensor([[[-2.7083e+00, -9.9991e+03, -3.2452e+00, -1.0000e+04,  3.3397e+00,\n",
       "            -9.9985e+03, -8.0218e-01, -9.9996e+03, -5.2809e-02, -9.9976e+03,\n",
       "            -1.6960e+00, -1.0000e+04,  8.0595e-01, -9.9994e+03, -1.1864e+00,\n",
       "            -9.9993e+03,  1.1634e+00, -9.9992e+03, -3.3863e+00, -1.0001e+04,\n",
       "             2.3295e+00, -1.0000e+04,  6.1551e-01, -1.0001e+04, -3.6707e-01,\n",
       "            -1.0001e+04, -9.2977e-01, -9.9997e+03,  4.8140e-01, -1.0001e+04,\n",
       "            -2.1461e+00, -9.9992e+03, -2.0133e+00, -9.9994e+03, -1.0128e+00,\n",
       "            -1.0001e+04, -4.6495e-02, -1.0000e+04,  2.6608e+00, -9.9999e+03,\n",
       "            -4.0778e-01, -9.9998e+03, -4.0299e+00, -1.0000e+04, -1.1274e-01,\n",
       "            -9.9996e+03, -9.5995e-02, -9.9989e+03,  1.9019e+00, -9.9996e+03,\n",
       "            -2.8813e+00, -9.9999e+03, -2.0686e-01, -9.9990e+03,  7.8721e-01,\n",
       "            -1.0000e+04, -5.6972e-02, -9.9993e+03,  9.7260e-01, -9.9993e+03,\n",
       "             2.6961e-01, -1.0000e+04, -6.3807e-01, -9.9986e+03, -3.3182e-01,\n",
       "            -9.9988e+03,  1.3031e+00, -9.9994e+03, -1.7175e+00, -1.0000e+04,\n",
       "             1.5349e+00, -1.0000e+04,  3.3656e-01],\n",
       "           [ 3.9506e+00, -3.1850e+00,  2.7766e+00, -2.2437e+00,  1.9414e+00,\n",
       "             2.6058e+00,  4.3507e+00, -1.0581e+00,  4.9277e+00,  2.4942e-01,\n",
       "             3.9684e+00, -4.1041e-01,  4.4808e+00,  4.8745e-01,  2.1423e+00,\n",
       "            -2.8703e+00,  1.8252e+00,  1.5172e+00,  4.4349e+00, -4.2964e+00,\n",
       "             2.5022e+00,  3.6286e+00,  2.4903e+00,  2.3199e+00,  3.3120e+00,\n",
       "             9.4923e-01,  2.7211e+00, -1.3316e+00,  1.1119e+00, -6.4689e-01,\n",
       "             3.7806e+00, -1.3759e+00,  4.5460e+00, -1.7022e+00,  2.7451e+00,\n",
       "            -1.3810e+00,  5.3357e+00, -1.2667e+00,  2.9722e+00,  1.6430e+00,\n",
       "             2.9706e+00, -1.9146e+00,  3.8188e+00, -4.1201e+00,  4.1310e+00,\n",
       "             4.0245e-01,  3.3434e+00,  1.2721e+00,  3.3955e+00,  2.7639e+00,\n",
       "             1.8882e+00, -2.4961e+00,  3.2956e+00, -2.9728e+00,  2.6526e+00,\n",
       "            -3.2439e-02,  5.4015e+00,  7.9554e-02,  3.8579e+00,  1.1408e+00,\n",
       "             3.2724e+00,  4.2409e-02,  2.6081e+00, -3.4444e-02,  2.8001e+00,\n",
       "            -1.3001e+00,  4.0114e+00,  1.0129e+00,  3.8594e+00, -3.6755e+00,\n",
       "             4.2252e+00,  1.3372e+00,  3.1746e+00],\n",
       "           [ 6.8817e+00,  3.2430e+00,  7.0519e+00,  3.0167e+00,  5.5971e+00,\n",
       "             2.1549e+00,  5.3380e+00,  6.1831e+00,  4.0592e+00,  6.2572e+00,\n",
       "             7.5862e+00,  3.0438e+00,  5.6113e+00,  4.5726e+00,  5.8939e+00,\n",
       "             7.2688e-01,  4.3928e+00,  1.5108e+00,  5.6217e+00,  5.4929e+00,\n",
       "             6.1268e+00,  2.4899e+00,  5.5302e+00,  4.2934e+00,  5.0309e+00,\n",
       "             3.6430e+00,  4.1737e+00,  1.5143e+00,  6.0572e+00,  5.5305e-01,\n",
       "             5.4539e+00,  3.6071e+00,  5.1344e+00,  3.0505e+00,  5.4219e+00,\n",
       "             2.0436e+00,  7.1689e+00,  4.0471e+00,  6.3308e+00,  4.4642e+00,\n",
       "             6.2559e+00,  2.2060e+00,  4.9614e+00,  4.6950e+00,  5.5345e+00,\n",
       "             2.5814e+00,  6.1700e+00,  4.2700e+00,  6.0700e+00,  3.4088e+00,\n",
       "             6.0534e+00,  1.5035e+00,  4.7369e+00,  2.5022e+00,  6.1463e+00,\n",
       "             3.0432e+00,  5.1663e+00,  6.2080e+00,  6.0195e+00,  3.3768e+00,\n",
       "             5.8595e+00,  5.0208e+00,  4.9635e+00,  1.8311e+00,  5.8215e+00,\n",
       "             1.5795e+00,  6.4201e+00,  3.8130e+00,  4.6299e+00,  3.9538e+00,\n",
       "             6.6723e+00,  5.2015e+00,  5.0227e+00],\n",
       "           [ 7.6506e+00,  6.3391e+00,  7.7652e+00,  7.7066e+00,  7.8201e+00,\n",
       "             6.3872e+00,  7.8022e+00,  4.8953e+00,  8.4059e+00,  6.3740e+00,\n",
       "             7.2167e+00,  6.6011e+00,  8.4091e+00,  7.0963e+00,  6.2861e+00,\n",
       "             6.7578e+00,  5.8856e+00,  3.4504e+00,  7.3655e+00,  4.9765e+00,\n",
       "             7.7664e+00,  5.2440e+00,  7.8424e+00,  5.3284e+00,  9.5092e+00,\n",
       "             6.2960e+00,  7.1272e+00,  4.7982e+00,  6.4253e+00,  6.3836e+00,\n",
       "             8.5326e+00,  5.6959e+00,  6.2057e+00,  4.1130e+00,  7.1173e+00,\n",
       "             6.0884e+00,  8.2972e+00,  8.6918e+00,  6.0760e+00,  7.4926e+00,\n",
       "             6.0846e+00,  7.5030e+00,  5.9113e+00,  6.2674e+00,  7.4168e+00,\n",
       "             6.4950e+00,  5.8591e+00,  6.5875e+00,  6.8680e+00,  6.7589e+00,\n",
       "             7.0104e+00,  7.5715e+00,  8.5226e+00,  5.6154e+00,  5.8775e+00,\n",
       "             7.6292e+00,  6.1088e+00,  4.1834e+00,  6.2738e+00,  6.5626e+00,\n",
       "             9.3475e+00,  5.0884e+00,  7.9689e+00,  4.6690e+00,  8.1949e+00,\n",
       "             5.6220e+00,  7.1533e+00,  5.9813e+00,  8.8492e+00,  4.2207e+00,\n",
       "             7.3243e+00,  3.9010e+00,  5.9820e+00],\n",
       "           [ 9.0559e+00,  7.5846e+00,  9.4925e+00,  7.3288e+00,  9.2227e+00,\n",
       "             8.8766e+00,  9.5802e+00,  8.3329e+00,  8.5558e+00,  7.2869e+00,\n",
       "             8.9086e+00,  7.2219e+00,  9.4214e+00,  8.6063e+00,  8.3405e+00,\n",
       "             5.3773e+00,  1.2041e+01,  5.8801e+00,  7.5361e+00,  7.7040e+00,\n",
       "             9.4551e+00,  8.6378e+00,  7.9220e+00,  7.3141e+00,  9.7961e+00,\n",
       "             1.1421e+01,  1.1418e+01,  6.7801e+00,  9.6571e+00,  6.3369e+00,\n",
       "             9.9916e+00,  8.5134e+00,  8.6309e+00,  4.7137e+00,  9.1834e+00,\n",
       "             6.1432e+00,  9.2321e+00,  9.5192e+00,  8.0384e+00,  8.3326e+00,\n",
       "             1.0611e+01,  7.9077e+00,  8.9746e+00,  4.2100e+00,  9.4701e+00,\n",
       "             8.7245e+00,  1.0141e+01,  6.0401e+00,  9.9614e+00,  6.7407e+00,\n",
       "             9.9405e+00,  7.9302e+00,  1.0141e+01,  8.4541e+00,  9.0311e+00,\n",
       "             7.4541e+00,  8.5561e+00,  4.4118e+00,  1.1142e+01,  6.8934e+00,\n",
       "             9.0095e+00,  9.0148e+00,  8.7664e+00,  6.3653e+00,  1.1417e+01,\n",
       "             8.2186e+00,  9.9501e+00,  7.4281e+00,  8.9581e+00,  9.0815e+00,\n",
       "             8.4983e+00,  6.2788e+00,  8.4498e+00],\n",
       "           [ 1.2252e+01,  7.1554e+00,  1.2215e+01,  1.0108e+01,  1.0724e+01,\n",
       "             9.6356e+00,  1.1966e+01,  8.7079e+00,  9.8852e+00,  7.9622e+00,\n",
       "             1.1052e+01,  9.2978e+00,  1.4389e+01,  9.1894e+00,  1.1246e+01,\n",
       "             6.5870e+00,  1.1258e+01,  1.2410e+01,  1.1752e+01,  6.7062e+00,\n",
       "             1.1543e+01,  8.8587e+00,  1.2426e+01,  7.5405e+00,  1.3039e+01,\n",
       "             9.6191e+00,  1.1103e+01,  1.2084e+01,  1.1101e+01,  8.4315e+00,\n",
       "             1.2134e+01,  8.7298e+00,  9.9388e+00,  8.8124e+00,  1.3077e+01,\n",
       "             7.6857e+00,  1.2769e+01,  8.8755e+00,  1.2386e+01,  8.7882e+00,\n",
       "             1.2020e+01,  1.0551e+01,  1.1655e+01,  9.7671e+00,  1.2009e+01,\n",
       "             9.4129e+00,  1.2335e+01,  1.1066e+01,  1.2098e+01,  1.0367e+01,\n",
       "             1.1744e+01,  9.5896e+00,  1.2252e+01,  9.4958e+00,  1.1430e+01,\n",
       "             7.3513e+00,  1.0492e+01,  7.9225e+00,  1.0291e+01,  1.1156e+01,\n",
       "             1.3848e+01,  8.7974e+00,  1.1458e+01,  9.4930e+00,  1.1765e+01,\n",
       "             8.5577e+00,  9.7723e+00,  9.5736e+00,  1.2996e+01,  7.9534e+00,\n",
       "             1.1300e+01,  8.9028e+00,  1.1717e+01],\n",
       "           [ 1.6794e+01,  1.1284e+01,  1.3691e+01,  1.3059e+01,  1.3403e+01,\n",
       "             1.1863e+01,  1.4513e+01,  1.1666e+01,  1.3440e+01,  9.2866e+00,\n",
       "             1.3735e+01,  1.1277e+01,  1.3339e+01,  1.3538e+01,  1.5428e+01,\n",
       "             1.1365e+01,  1.3341e+01,  1.2445e+01,  1.3692e+01,  1.1891e+01,\n",
       "             1.6245e+01,  1.0585e+01,  1.3290e+01,  1.2524e+01,  1.3503e+01,\n",
       "             1.2335e+01,  1.3881e+01,  1.2619e+01,  1.3513e+01,  1.0308e+01,\n",
       "             1.2888e+01,  1.0103e+01,  1.4950e+01,  8.7139e+00,  1.4992e+01,\n",
       "             1.3147e+01,  1.3834e+01,  1.1827e+01,  1.4345e+01,  1.3804e+01,\n",
       "             1.4506e+01,  1.1593e+01,  1.4623e+01,  1.1211e+01,  1.5744e+01,\n",
       "             1.1546e+01,  1.3875e+01,  1.4042e+01,  1.3879e+01,  1.2117e+01,\n",
       "             1.4837e+01,  1.0654e+01,  1.4652e+01,  1.2443e+01,  1.4557e+01,\n",
       "             1.2150e+01,  1.5406e+01,  1.0689e+01,  1.4158e+01,  8.8819e+00,\n",
       "             1.3693e+01,  1.3455e+01,  1.4962e+01,  1.1843e+01,  1.3808e+01,\n",
       "             1.0692e+01,  1.3245e+01,  9.4470e+00,  1.3696e+01,  1.3256e+01,\n",
       "             1.4300e+01,  1.3938e+01,  1.4479e+01],\n",
       "           [ 1.7243e+01,  1.7455e+01,  1.7140e+01,  1.4000e+01,  1.7125e+01,\n",
       "             1.4306e+01,  1.7469e+01,  1.5001e+01,  1.7058e+01,  1.4395e+01,\n",
       "             1.7861e+01,  1.5371e+01,  1.5243e+01,  1.3869e+01,  1.7448e+01,\n",
       "             1.5542e+01,  1.7341e+01,  1.3467e+01,  1.6902e+01,  1.3546e+01,\n",
       "             1.6337e+01,  1.8138e+01,  1.7240e+01,  1.2511e+01,  1.8371e+01,\n",
       "             1.2679e+01,  1.4986e+01,  1.4533e+01,  1.4884e+01,  1.5617e+01,\n",
       "             1.6148e+01,  1.3706e+01,  1.8245e+01,  1.4450e+01,  1.7148e+01,\n",
       "             1.5513e+01,  1.5334e+01,  1.4732e+01,  1.7438e+01,  1.6023e+01,\n",
       "             1.8392e+01,  1.5646e+01,  1.6578e+01,  1.2823e+01,  1.6072e+01,\n",
       "             1.6188e+01,  1.6351e+01,  1.4029e+01,  1.6021e+01,  1.3953e+01,\n",
       "             1.7438e+01,  1.5123e+01,  1.6011e+01,  1.2960e+01,  1.6512e+01,\n",
       "             1.6410e+01,  1.8341e+01,  1.6806e+01,  1.7702e+01,  1.4517e+01,\n",
       "             1.6630e+01,  1.3812e+01,  1.7061e+01,  1.4869e+01,  1.7807e+01,\n",
       "             1.5940e+01,  1.6867e+01,  1.3555e+01,  1.7109e+01,  1.3004e+01,\n",
       "             1.6559e+01,  1.3399e+01,  1.9070e+01],\n",
       "           [ 1.8395e+01,  1.5746e+01,  1.9642e+01,  1.7615e+01,  1.7482e+01,\n",
       "             1.6441e+01,  1.8591e+01,  1.9215e+01,  1.9387e+01,  1.5711e+01,\n",
       "             1.8954e+01,  1.7530e+01,  1.9424e+01,  1.6101e+01,  2.0061e+01,\n",
       "             1.6042e+01,  1.9881e+01,  1.7368e+01,  1.8932e+01,  1.7331e+01,\n",
       "             1.7318e+01,  1.8604e+01,  1.8003e+01,  1.6456e+01,  1.7877e+01,\n",
       "             1.7566e+01,  2.0003e+01,  1.5362e+01,  2.1195e+01,  1.5686e+01,\n",
       "             1.9526e+01,  1.5999e+01,  1.8957e+01,  1.6244e+01,  2.0421e+01,\n",
       "             1.6827e+01,  1.7770e+01,  1.5855e+01,  1.7898e+01,  1.7178e+01,\n",
       "             1.8725e+01,  1.8589e+01,  1.8561e+01,  1.6757e+01,  1.8438e+01,\n",
       "             1.7021e+01,  2.1096e+01,  1.5739e+01,  1.8478e+01,  1.4639e+01,\n",
       "             1.8678e+01,  1.6862e+01,  1.7151e+01,  1.7395e+01,  1.8004e+01,\n",
       "             1.6267e+01,  1.9569e+01,  1.8666e+01,  2.0655e+01,  1.9490e+01,\n",
       "             1.9084e+01,  1.8158e+01,  1.7204e+01,  1.6494e+01,  1.9980e+01,\n",
       "             1.8471e+01,  1.9218e+01,  1.8352e+01,  1.9683e+01,  1.6480e+01,\n",
       "             1.8763e+01,  1.6211e+01,  1.9859e+01],\n",
       "           [ 2.0891e+01,  1.9952e+01,  2.1711e+01,  1.9567e+01,  2.2874e+01,\n",
       "             1.8693e+01,  1.9891e+01,  1.7908e+01,  2.2577e+01,  1.6763e+01,\n",
       "             1.9428e+01,  1.8825e+01,  1.7591e+01,  2.0048e+01,  2.0816e+01,\n",
       "             2.1731e+01,  2.1158e+01,  1.8425e+01,  1.8174e+01,  1.8572e+01,\n",
       "             2.1674e+01,  1.7000e+01,  2.2453e+01,  1.9701e+01,  2.0732e+01,\n",
       "             2.0362e+01,  2.0665e+01,  1.9977e+01,  2.2894e+01,  2.2470e+01,\n",
       "             2.1993e+01,  2.0111e+01,  2.0004e+01,  1.6162e+01,  1.9162e+01,\n",
       "             1.8656e+01,  2.1877e+01,  1.8402e+01,  2.1678e+01,  1.7923e+01,\n",
       "             2.1009e+01,  2.1280e+01,  2.1768e+01,  1.7127e+01,  2.1587e+01,\n",
       "             1.7758e+01,  2.2017e+01,  2.2394e+01,  2.0120e+01,  1.8094e+01,\n",
       "             2.1176e+01,  1.9065e+01,  1.9339e+01,  1.9011e+01,  2.0206e+01,\n",
       "             1.9244e+01,  2.0066e+01,  2.0066e+01,  2.1979e+01,  2.2619e+01,\n",
       "             2.0788e+01,  1.9157e+01,  1.8201e+01,  1.8134e+01,  2.4495e+01,\n",
       "             1.7512e+01,  2.0121e+01,  1.8636e+01,  2.1297e+01,  1.9736e+01,\n",
       "             2.0861e+01,  1.6759e+01,  2.0106e+01]]]),\n",
       "  tensor([[[-1.0002e+04, -2.0001e+04, -9.9991e+03, -2.0001e+04,  4.5552e+00,\n",
       "            -9.9992e+03, -9.9989e+03, -2.0001e+04, -1.0000e+04, -1.9999e+04,\n",
       "            -1.0000e+04, -2.0000e+04, -9.9994e+03, -2.0000e+04, -1.0001e+04,\n",
       "            -2.0001e+04, -1.0001e+04, -2.0000e+04, -1.0004e+04, -2.0000e+04,\n",
       "            -9.9996e+03, -2.0000e+04, -1.0001e+04, -2.0000e+04, -1.0001e+04,\n",
       "            -2.0001e+04, -1.0000e+04, -1.9999e+04, -9.9972e+03, -2.0000e+04,\n",
       "            -1.0001e+04, -2.0000e+04, -9.9998e+03, -2.0000e+04, -9.9995e+03,\n",
       "            -2.0000e+04, -1.0000e+04, -2.0000e+04, -9.9993e+03, -2.0000e+04,\n",
       "            -9.9996e+03, -2.0000e+04, -1.0002e+04, -2.0000e+04, -1.0001e+04,\n",
       "            -1.9998e+04, -1.0002e+04, -2.0000e+04, -1.0000e+04, -1.9999e+04,\n",
       "            -1.0001e+04, -1.9999e+04, -1.0002e+04, -2.0000e+04, -1.0000e+04,\n",
       "            -2.0000e+04, -9.9998e+03, -2.0001e+04, -1.0001e+04, -2.0000e+04,\n",
       "            -1.0000e+04, -2.0000e+04, -1.0003e+04, -2.0001e+04, -1.0001e+04,\n",
       "            -2.0001e+04, -9.9988e+03, -2.0000e+04, -9.9994e+03, -1.9998e+04,\n",
       "            -1.0000e+04, -1.9999e+04, -1.0000e+04],\n",
       "           [ 4.3512e+00, -2.6976e+00,  9.7035e-01,  2.6847e-02,  3.0122e+00,\n",
       "             6.2155e+00,  4.1867e+00,  1.3301e+00,  4.6179e+00, -9.9691e-01,\n",
       "             4.7128e+00,  8.3537e-02,  3.7316e+00,  2.6236e-01,  2.5796e+00,\n",
       "            -3.1855e-01,  3.5043e+00, -7.5268e-01,  4.9744e+00, -1.9152e+00,\n",
       "             4.4255e+00,  2.6296e-01,  4.9031e+00, -2.1900e+00,  2.1348e+00,\n",
       "            -1.1137e+00,  3.4415e+00, -6.6524e-01,  4.3073e+00,  2.4560e+00,\n",
       "             3.3350e+00, -8.2883e-01,  1.6854e+00,  4.5451e-01,  3.0479e+00,\n",
       "             3.1458e-01,  3.4186e+00, -9.9211e-01,  5.2199e+00,  1.8451e+00,\n",
       "             3.8332e+00,  2.9470e-01,  3.2013e+00, -5.6338e-02,  5.3139e+00,\n",
       "            -2.7245e+00,  5.3597e+00, -2.6457e+00,  3.8810e+00,  5.7277e-01,\n",
       "             3.6990e+00,  1.1470e+00,  4.9995e+00, -1.5996e-01,  4.2702e+00,\n",
       "             1.4880e-01,  4.6270e+00, -6.7153e-02,  4.4358e+00,  5.4445e-01,\n",
       "             4.7972e+00, -1.1443e+00,  5.7344e+00, -3.4003e+00,  3.9086e+00,\n",
       "            -1.0419e+00,  3.9787e+00,  1.7877e+00,  1.5785e+00,  3.5816e-01,\n",
       "             4.6492e+00, -1.9718e-01,  3.8925e+00],\n",
       "           [ 4.4536e+00,  4.7456e+00,  6.2819e+00,  1.5210e-01,  6.0389e+00,\n",
       "             5.5373e+00,  4.7598e+00,  3.1836e+00,  5.5849e+00,  3.0329e+00,\n",
       "             5.7221e+00,  6.3009e+00,  5.5297e+00,  4.3441e+00,  6.1554e+00,\n",
       "             2.8924e+00,  8.3015e+00,  5.7095e+00,  6.3180e+00,  4.4288e+00,\n",
       "             6.9411e+00,  5.3680e+00,  5.6975e+00,  6.4909e+00,  7.2806e+00,\n",
       "             3.0276e+00,  3.5820e+00,  4.2914e+00,  5.9796e+00,  5.0796e+00,\n",
       "             4.4440e+00,  5.5325e+00,  6.0823e+00,  7.1611e-01,  6.5207e+00,\n",
       "             2.0917e+00,  5.7703e+00,  3.5669e+00,  7.8410e+00,  3.4490e+00,\n",
       "             4.0786e+00,  5.0729e+00,  3.8242e+00,  3.1857e+00,  5.7541e+00,\n",
       "             4.1131e+00,  4.7282e+00,  5.7192e+00,  6.0796e+00,  4.9129e+00,\n",
       "             4.0987e+00,  3.8100e+00,  7.9028e+00,  3.5613e+00,  6.5174e+00,\n",
       "             4.6523e+00,  5.7286e+00,  3.4492e+00,  6.4863e+00,  6.4215e+00,\n",
       "             6.7713e+00,  4.2216e+00,  4.1764e+00,  5.1697e+00,  5.5254e+00,\n",
       "             5.3305e+00,  6.3167e+00,  4.7037e+00,  4.2210e+00,  4.7993e-01,\n",
       "             6.3076e+00,  4.7169e+00,  6.7525e+00],\n",
       "           [ 6.7305e+00,  5.4031e+00,  7.7826e+00,  6.4773e+00,  7.2789e+00,\n",
       "             5.8974e+00,  7.3999e+00,  7.1533e+00,  7.6952e+00,  5.1008e+00,\n",
       "             9.3676e+00,  5.1602e+00,  6.4162e+00,  5.0171e+00,  7.6618e+00,\n",
       "             7.1986e+00,  8.1130e+00,  9.4644e+00,  7.6630e+00,  8.0581e+00,\n",
       "             1.0404e+01,  6.2751e+00,  7.9164e+00,  6.9930e+00,  7.2557e+00,\n",
       "             7.3295e+00,  7.4565e+00,  4.9087e+00,  7.2035e+00,  4.3046e+00,\n",
       "             8.8132e+00,  5.6617e+00,  6.6123e+00,  6.6344e+00,  7.3929e+00,\n",
       "             7.5819e+00,  7.9218e+00,  5.5787e+00,  8.7012e+00,  7.9980e+00,\n",
       "             7.2774e+00,  5.8367e+00,  8.7424e+00,  3.9724e+00,  1.0352e+01,\n",
       "             4.5512e+00,  7.9512e+00,  3.6238e+00,  9.2670e+00,  4.7865e+00,\n",
       "             8.2448e+00,  4.1184e+00,  7.7959e+00,  8.4023e+00,  7.5433e+00,\n",
       "             6.8530e+00,  8.0073e+00,  6.2853e+00,  7.9013e+00,  5.2435e+00,\n",
       "             9.6991e+00,  7.1204e+00,  8.1451e+00,  5.7060e+00,  9.3559e+00,\n",
       "             3.8896e+00,  7.3851e+00,  7.0367e+00,  8.9278e+00,  4.8367e+00,\n",
       "             9.2183e+00,  5.3326e+00,  8.9882e+00],\n",
       "           [ 1.0754e+01,  6.0286e+00,  1.1402e+01,  6.6016e+00,  1.1199e+01,\n",
       "             8.3351e+00,  1.0365e+01,  8.4008e+00,  9.3527e+00,  7.7213e+00,\n",
       "             9.9636e+00,  1.0471e+01,  9.8377e+00,  6.6902e+00,  9.4649e+00,\n",
       "             7.3638e+00,  1.1152e+01,  8.3971e+00,  1.0308e+01,  8.8665e+00,\n",
       "             1.1178e+01,  1.0101e+01,  1.2156e+01,  6.6441e+00,  9.9483e+00,\n",
       "             7.5756e+00,  9.3909e+00,  8.2245e+00,  9.7182e+00,  6.3234e+00,\n",
       "             9.1117e+00,  1.1147e+01,  1.1103e+01,  6.5590e+00,  9.8885e+00,\n",
       "             6.9577e+00,  1.1658e+01,  7.8560e+00,  8.9834e+00,  8.8063e+00,\n",
       "             1.1575e+01,  6.1103e+00,  1.1979e+01,  1.0743e+01,  1.1847e+01,\n",
       "             9.2564e+00,  1.1140e+01,  6.1670e+00,  9.8550e+00,  9.4054e+00,\n",
       "             1.0367e+01,  8.4873e+00,  9.6463e+00,  8.6586e+00,  1.2099e+01,\n",
       "             8.5546e+00,  1.0106e+01,  8.1867e+00,  1.0874e+01,  7.7813e+00,\n",
       "             1.0190e+01,  1.0695e+01,  8.7141e+00,  8.1554e+00,  1.1343e+01,\n",
       "             7.4934e+00,  1.1416e+01,  5.0342e+00,  1.0081e+01,  8.2738e+00,\n",
       "             1.0798e+01,  8.4138e+00,  1.1726e+01],\n",
       "           [ 1.1251e+01,  1.1825e+01,  1.2829e+01,  1.1343e+01,  1.2301e+01,\n",
       "             1.3234e+01,  1.0733e+01,  1.0951e+01,  1.3807e+01,  1.0711e+01,\n",
       "             1.3690e+01,  9.4092e+00,  1.3256e+01,  1.0951e+01,  1.3493e+01,\n",
       "             1.0149e+01,  1.2297e+01,  1.0692e+01,  1.2680e+01,  1.0701e+01,\n",
       "             1.3339e+01,  1.0135e+01,  1.2975e+01,  1.2868e+01,  1.0664e+01,\n",
       "             9.5194e+00,  1.3331e+01,  9.0185e+00,  1.2421e+01,  1.2008e+01,\n",
       "             1.2211e+01,  9.8167e+00,  1.2296e+01,  1.1269e+01,  1.2779e+01,\n",
       "             1.0380e+01,  1.2865e+01,  1.2443e+01,  1.2484e+01,  1.0563e+01,\n",
       "             1.1809e+01,  1.1503e+01,  1.1114e+01,  1.2187e+01,  1.1765e+01,\n",
       "             1.1006e+01,  1.1086e+01,  1.0359e+01,  1.1871e+01,  1.0265e+01,\n",
       "             1.3494e+01,  8.8682e+00,  1.0636e+01,  1.0366e+01,  1.2459e+01,\n",
       "             1.1865e+01,  1.3083e+01,  1.2073e+01,  1.2362e+01,  1.1981e+01,\n",
       "             1.0914e+01,  1.2533e+01,  1.3962e+01,  7.3988e+00,  1.2052e+01,\n",
       "             1.1624e+01,  1.3659e+01,  1.2445e+01,  1.2964e+01,  9.8084e+00,\n",
       "             1.3618e+01,  1.2381e+01,  1.2630e+01],\n",
       "           [ 1.3211e+01,  1.2208e+01,  1.3618e+01,  1.3477e+01,  1.4784e+01,\n",
       "             1.2086e+01,  1.3596e+01,  1.1385e+01,  1.3450e+01,  1.4520e+01,\n",
       "             1.4035e+01,  1.4492e+01,  1.4463e+01,  1.1456e+01,  1.4639e+01,\n",
       "             1.2465e+01,  1.4896e+01,  1.2701e+01,  1.3480e+01,  1.2886e+01,\n",
       "             1.4040e+01,  1.1658e+01,  1.5027e+01,  1.3504e+01,  1.4095e+01,\n",
       "             1.0740e+01,  1.4784e+01,  1.1974e+01,  1.4525e+01,  1.1175e+01,\n",
       "             1.4623e+01,  1.3324e+01,  1.4694e+01,  1.0591e+01,  1.3105e+01,\n",
       "             1.3394e+01,  1.3676e+01,  1.2328e+01,  1.3574e+01,  1.1187e+01,\n",
       "             1.2163e+01,  1.3201e+01,  1.5375e+01,  1.2355e+01,  1.2631e+01,\n",
       "             1.2186e+01,  1.3650e+01,  9.0081e+00,  1.2777e+01,  1.1371e+01,\n",
       "             1.4290e+01,  1.4468e+01,  1.3952e+01,  1.0070e+01,  1.4398e+01,\n",
       "             1.3273e+01,  1.5819e+01,  1.4300e+01,  1.4493e+01,  1.2295e+01,\n",
       "             1.3526e+01,  1.1509e+01,  1.4296e+01,  1.4286e+01,  1.4524e+01,\n",
       "             1.1928e+01,  1.4133e+01,  1.3070e+01,  1.3927e+01,  1.2431e+01,\n",
       "             1.5207e+01,  1.3606e+01,  1.3604e+01],\n",
       "           [ 1.4408e+01,  1.4584e+01,  1.2913e+01,  1.2679e+01,  1.6493e+01,\n",
       "             1.5018e+01,  1.7445e+01,  1.3322e+01,  1.6558e+01,  1.4653e+01,\n",
       "             1.5732e+01,  1.3925e+01,  1.6713e+01,  1.5158e+01,  1.6104e+01,\n",
       "             1.4309e+01,  1.5921e+01,  1.4792e+01,  1.5854e+01,  1.3612e+01,\n",
       "             1.5770e+01,  1.4631e+01,  1.5869e+01,  1.6212e+01,  1.6486e+01,\n",
       "             1.4173e+01,  1.6737e+01,  1.4934e+01,  1.8084e+01,  1.3660e+01,\n",
       "             1.5086e+01,  1.4763e+01,  1.6026e+01,  1.4725e+01,  1.5674e+01,\n",
       "             1.2602e+01,  1.5380e+01,  1.4831e+01,  1.6049e+01,  1.2721e+01,\n",
       "             1.4120e+01,  1.2956e+01,  1.6190e+01,  1.6535e+01,  1.4684e+01,\n",
       "             1.3545e+01,  1.4870e+01,  1.2614e+01,  1.4132e+01,  1.2778e+01,\n",
       "             1.4456e+01,  1.4554e+01,  1.4281e+01,  1.4487e+01,  1.5531e+01,\n",
       "             1.2454e+01,  1.7923e+01,  1.5329e+01,  1.6763e+01,  1.4449e+01,\n",
       "             1.6469e+01,  1.4812e+01,  1.5148e+01,  1.4696e+01,  1.5311e+01,\n",
       "             1.4428e+01,  1.7312e+01,  1.5197e+01,  1.6120e+01,  1.3478e+01,\n",
       "             1.4919e+01,  1.4520e+01,  1.5203e+01],\n",
       "           [ 1.6592e+01,  1.5190e+01,  1.7164e+01,  1.0596e+01,  1.8599e+01,\n",
       "             1.6652e+01,  1.7315e+01,  1.6614e+01,  1.6752e+01,  1.6581e+01,\n",
       "             1.7832e+01,  1.4202e+01,  1.6811e+01,  1.6455e+01,  1.7908e+01,\n",
       "             1.4664e+01,  1.7984e+01,  1.6514e+01,  2.0697e+01,  1.6218e+01,\n",
       "             1.7381e+01,  1.3816e+01,  1.8543e+01,  1.5633e+01,  1.6929e+01,\n",
       "             1.6381e+01,  1.5580e+01,  1.5662e+01,  1.8164e+01,  1.8284e+01,\n",
       "             1.8817e+01,  1.5184e+01,  1.9272e+01,  1.7055e+01,  1.8791e+01,\n",
       "             1.6931e+01,  1.9312e+01,  1.7052e+01,  1.7584e+01,  1.5853e+01,\n",
       "             1.7505e+01,  1.3623e+01,  1.7991e+01,  1.4784e+01,  1.8421e+01,\n",
       "             1.3852e+01,  1.6533e+01,  1.3547e+01,  1.7661e+01,  1.4138e+01,\n",
       "             1.8505e+01,  1.2374e+01,  1.8965e+01,  1.4804e+01,  1.7375e+01,\n",
       "             1.6468e+01,  1.8587e+01,  1.7915e+01,  1.7183e+01,  1.7547e+01,\n",
       "             1.9349e+01,  1.6493e+01,  1.5773e+01,  1.5744e+01,  1.7460e+01,\n",
       "             1.5229e+01,  1.9593e+01,  1.7455e+01,  1.5465e+01,  1.6995e+01,\n",
       "             1.8885e+01,  1.5372e+01,  1.6518e+01],\n",
       "           [ 2.0765e+01,  1.5975e+01,  1.9100e+01,  1.6713e+01,  2.1008e+01,\n",
       "             1.7285e+01,  1.8659e+01,  1.6327e+01,  2.2205e+01,  1.3600e+01,\n",
       "             1.9744e+01,  1.6460e+01,  1.8397e+01,  1.6924e+01,  1.8664e+01,\n",
       "             1.8033e+01,  2.0287e+01,  1.6504e+01,  1.8030e+01,  2.2313e+01,\n",
       "             2.3204e+01,  1.6668e+01,  1.9648e+01,  1.9829e+01,  1.9753e+01,\n",
       "             1.8023e+01,  2.0762e+01,  1.7250e+01,  2.1669e+01,  2.0188e+01,\n",
       "             2.0143e+01,  1.6848e+01,  1.9879e+01,  1.8664e+01,  1.8628e+01,\n",
       "             1.8876e+01,  2.0841e+01,  1.9292e+01,  2.0076e+01,  2.0143e+01,\n",
       "             2.0189e+01,  1.6349e+01,  1.9437e+01,  1.6836e+01,  2.2823e+01,\n",
       "             1.7570e+01,  1.8973e+01,  1.7533e+01,  1.8562e+01,  1.7937e+01,\n",
       "             2.0756e+01,  1.8033e+01,  1.9780e+01,  2.0105e+01,  1.8846e+01,\n",
       "             1.7020e+01,  2.0154e+01,  1.7480e+01,  2.2358e+01,  1.8381e+01,\n",
       "             2.0525e+01,  1.9613e+01,  1.7800e+01,  1.5978e+01,  2.1440e+01,\n",
       "             1.5155e+01,  2.0260e+01,  1.8061e+01,  2.1332e+01,  1.8066e+01,\n",
       "             2.1472e+01,  1.9177e+01,  2.0430e+01]]])])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf.viterbi_tags(torch.randn([2, 10, 73]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6ec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2287e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp.nn.util as util\n",
    "def viterbi_tags(\n",
    "    logits: torch.Tensor, mask: torch.BoolTensor = None, top_k: int = None,\n",
    "    _constraint_mask= None, _transitions=None, start_transitions=None, end_transitions=None\n",
    "):\n",
    "    if mask is None:\n",
    "        mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)\n",
    "\n",
    "    if top_k is None:\n",
    "        top_k = 1\n",
    "        flatten_output = True\n",
    "    else:\n",
    "        flatten_output = False\n",
    "    all_prob_val = []\n",
    "    _, max_seq_length, num_tags = logits.size()\n",
    "#     print(num_tags)\n",
    "    # Get the tensors out of the variables\n",
    "    logits, mask = logits.data, mask.data\n",
    "\n",
    "    # Augment transitions matrix with start and end transitions\n",
    "    start_tag = num_tags\n",
    "    end_tag = num_tags + 1\n",
    "    transitions = torch.full((num_tags + 2, num_tags + 2), -10000.0, device=logits.device)\n",
    "\n",
    "    # Apply transition constraints\n",
    "    constrained_transitions = _transitions * _constraint_mask[:num_tags, :num_tags] + -10000.0 * (1 - _constraint_mask[:num_tags, :num_tags])\n",
    "    transitions[:num_tags, :num_tags] = constrained_transitions.data\n",
    "\n",
    "    if True:\n",
    "        transitions[\n",
    "            start_tag, :num_tags\n",
    "        ] = start_transitions.detach() * _constraint_mask[\n",
    "            start_tag, :num_tags\n",
    "        ].data + -10000.0 * (\n",
    "            1 - _constraint_mask[start_tag, :num_tags].detach()\n",
    "        )\n",
    "        transitions[:num_tags, end_tag] = end_transitions.detach() * _constraint_mask[\n",
    "            :num_tags, end_tag\n",
    "        ].data + -10000.0 * (1 - _constraint_mask[:num_tags, end_tag].detach())\n",
    "    else:\n",
    "        transitions[start_tag, :num_tags] = -10000.0 * (\n",
    "            1 - self._constraint_mask[start_tag, :num_tags].detach()\n",
    "        )\n",
    "        transitions[:num_tags, end_tag] = -10000.0 * (\n",
    "            1 - self._constraint_mask[:num_tags, end_tag].detach()\n",
    "        )\n",
    "\n",
    "    best_paths = []\n",
    "    # Pad the max sequence length by 2 to account for start_tag + end_tag.\n",
    "    tag_sequence = torch.empty(max_seq_length + 2, num_tags + 2, device=logits.device)\n",
    "\n",
    "    for prediction, prediction_mask in zip(logits, mask):\n",
    "        mask_indices = prediction_mask.nonzero(as_tuple=False).squeeze()\n",
    "        masked_prediction = torch.index_select(prediction, 0, mask_indices)\n",
    "        sequence_length = masked_prediction.shape[0]\n",
    "\n",
    "        # Start with everything totally unlikely\n",
    "        tag_sequence.fill_(-10000.0)\n",
    "        # At timestep 0 we must have the START_TAG\n",
    "        tag_sequence[0, start_tag] = 0.0\n",
    "        # At steps 1, ..., sequence_length we just use the incoming prediction\n",
    "        tag_sequence[1 : (sequence_length + 1), :num_tags] = masked_prediction\n",
    "        # And at the last timestep we must have the END_TAG\n",
    "        tag_sequence[sequence_length + 1, end_tag] = 0.0\n",
    "#         print(torch.argmax(tag_sequence, dim=-1))\n",
    "\n",
    "        # We pass the tags and the transitions to `viterbi_decode`.\n",
    "        viterbi_paths, viterbi_scores, all_prob = viterbi_decode(\n",
    "            tag_sequence=tag_sequence[: (sequence_length + 2)],\n",
    "            transition_matrix=transitions,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        stacked_t = torch.stack(list(reversed(all_prob))[1:])\n",
    "        all_prob_val.append(stacked_t.unsqueeze(0))\n",
    "#         print(viterbi_paths, viterbi_scores)\n",
    "        top_k_paths = []\n",
    "        for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):\n",
    "            # Get rid of START and END sentinels and append.\n",
    "            viterbi_path = viterbi_path[1:-1]\n",
    "            top_k_paths.append((viterbi_path, viterbi_score.item()))\n",
    "        best_paths.append(top_k_paths)\n",
    "\n",
    "    if flatten_output:\n",
    "        return [top_k_paths[0] for top_k_paths in best_paths]\n",
    "\n",
    "    return best_paths, all_prob_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93da05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "def viterbi_decode(\n",
    "    tag_sequence: torch.Tensor,\n",
    "    transition_matrix: torch.Tensor,\n",
    "    tag_observations: Optional[List[int]] = None,\n",
    "    allowed_start_transitions: torch.Tensor = None,\n",
    "    allowed_end_transitions: torch.Tensor = None,\n",
    "    top_k: int = None,\n",
    "):\n",
    "    global all_score\n",
    "    all_prob = []\n",
    "#     print(tag_sequence.shape)\n",
    "    if top_k is None:\n",
    "        top_k = 1\n",
    "        flatten_output = True\n",
    "    elif top_k >= 1:\n",
    "        flatten_output = False\n",
    "    else:\n",
    "        raise ValueError(f\"top_k must be either None or an integer >=1. Instead received {top_k}\")\n",
    "\n",
    "    sequence_length, num_tags = list(tag_sequence.size())\n",
    "\n",
    "    has_start_end_restrictions = (\n",
    "        allowed_end_transitions is not None or allowed_start_transitions is not None\n",
    "    )\n",
    "\n",
    "    if has_start_end_restrictions:\n",
    "\n",
    "        if allowed_end_transitions is None:\n",
    "            allowed_end_transitions = torch.zeros(num_tags)\n",
    "        if allowed_start_transitions is None:\n",
    "            allowed_start_transitions = torch.zeros(num_tags)\n",
    "\n",
    "        num_tags = num_tags + 2\n",
    "        new_transition_matrix = torch.zeros(num_tags, num_tags)\n",
    "        new_transition_matrix[:-2, :-2] = transition_matrix\n",
    "\n",
    "        # Start and end transitions are fully defined, but cannot transition between each other.\n",
    "\n",
    "        allowed_start_transitions = torch.cat(\n",
    "            [allowed_start_transitions, torch.tensor([-math.inf, -math.inf])]\n",
    "        )\n",
    "        allowed_end_transitions = torch.cat(\n",
    "            [allowed_end_transitions, torch.tensor([-math.inf, -math.inf])]\n",
    "        )\n",
    "\n",
    "        # First define how we may transition FROM the start and end tags.\n",
    "        new_transition_matrix[-2, :] = allowed_start_transitions\n",
    "        # We cannot transition from the end tag to any tag.\n",
    "        new_transition_matrix[-1, :] = -math.inf\n",
    "\n",
    "        new_transition_matrix[:, -1] = allowed_end_transitions\n",
    "        # We cannot transition to the start tag from any tag.\n",
    "        new_transition_matrix[:, -2] = -math.inf\n",
    "\n",
    "        transition_matrix = new_transition_matrix\n",
    "\n",
    "    if tag_observations:\n",
    "        if len(tag_observations) != sequence_length:\n",
    "            raise ConfigurationError(\n",
    "                \"Observations were provided, but they were not the same length \"\n",
    "                \"as the sequence. Found sequence of length: {} and evidence: {}\".format(\n",
    "                    sequence_length, tag_observations\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        tag_observations = [-1 for _ in range(sequence_length)]\n",
    "\n",
    "    if has_start_end_restrictions:\n",
    "        tag_observations = [num_tags - 2] + tag_observations + [num_tags - 1]\n",
    "        zero_sentinel = torch.zeros(1, num_tags)\n",
    "        extra_tags_sentinel = torch.ones(sequence_length, 2) * -math.inf\n",
    "        tag_sequence = torch.cat([tag_sequence, extra_tags_sentinel], -1)\n",
    "        tag_sequence = torch.cat([zero_sentinel, tag_sequence, zero_sentinel], 0)\n",
    "        sequence_length = tag_sequence.size(0)\n",
    "\n",
    "    path_scores = []\n",
    "    path_indices = []\n",
    "\n",
    "    if tag_observations[0] != -1:\n",
    "        one_hot = torch.zeros(num_tags)\n",
    "        one_hot[tag_observations[0]] = 100000.0\n",
    "        path_scores.append(one_hot.unsqueeze(0))\n",
    "    else:\n",
    "        path_scores.append(tag_sequence[0, :].unsqueeze(0))\n",
    "\n",
    "    # Evaluate the scores for all possible paths.\n",
    "#     print(\"length\", path_scores[0].shape)\n",
    "#     all_score = []\n",
    "    for timestep in range(1, sequence_length):\n",
    "#         print(timestep, path_scores[timestep - 1].shape, transition_matrix.shape)\n",
    "        # Add pairwise potentials to current scores.\n",
    "        summed_potentials = path_scores[timestep - 1].unsqueeze(2) + transition_matrix\n",
    "        summed_potentials = summed_potentials.view(-1, num_tags)\n",
    "#         print(summed_potentials.shape, num_tags)\n",
    "        # Best pairwise potential path score from the previous timestep.\n",
    "        max_k = min(summed_potentials.size()[0], top_k)\n",
    "        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)\n",
    "#         print(scores, paths)\n",
    "        all_score.append(summed_potentials)\n",
    "#         print(\"best paths\", paths)\n",
    "        # If we have an observation for this timestep, use it\n",
    "        # instead of the distribution over tags.\n",
    "        observation = tag_observations[timestep]\n",
    "        # Warn the user if they have passed\n",
    "        # invalid/extremely unlikely evidence.\n",
    "        if tag_observations[timestep - 1] != -1 and observation != -1:\n",
    "            if transition_matrix[tag_observations[timestep - 1], observation] < -10000:\n",
    "                logger.warning(\n",
    "                    \"The pairwise potential between tags you have passed as \"\n",
    "                    \"observations is extremely unlikely. Double check your evidence \"\n",
    "                    \"or transition potentials!\"\n",
    "                )\n",
    "        if observation != -1:\n",
    "            one_hot = torch.zeros(num_tags)\n",
    "            one_hot[observation] = 100000.0\n",
    "            path_scores.append(one_hot.unsqueeze(0))\n",
    "        else:\n",
    "            path_scores.append(tag_sequence[timestep, :] + scores)\n",
    "        path_indices.append(paths.squeeze())\n",
    "#     print(\"Simple Argmax\", torch.argmax(torch.cat(path_scores[1:-1], dim=0), dim=-1))\n",
    "#     print(torch.argmax(torch.cat(all_score, dim=0), dim=-1))\n",
    "    # Construct the most likely sequence backwards.\n",
    "    path_scores_v = path_scores[-1].view(-1)\n",
    "#     print(path_scores[-1].shape)\n",
    "    max_k = min(path_scores_v.size()[0], top_k)\n",
    "    viterbi_scores, best_paths = torch.topk(path_scores_v, k=max_k, dim=0)\n",
    "#     print(viterbi_scores, best_paths, max_k)\n",
    "    viterbi_paths = []\n",
    "    \n",
    "    for i in range(max_k):\n",
    "        viterbi_path = [best_paths[i]]\n",
    "        \n",
    "        for idx, backward_timestep in enumerate(reversed(path_indices)):\n",
    "#             print(idx)\n",
    "#             print(\"All Collected\", backward_timestep, viterbi_path[-1])\n",
    "#             print(\"recreated\", all_score[-(idx+1)])\n",
    "#             print(\"Tensor\", all_score[-(idx+1)][:, viterbi_path[-1]])\n",
    "            all_prob.append(all_score[-(idx+1)][:-2][:, viterbi_path[-1]])\n",
    "            viterbi_path.append(int(backward_timestep.view(-1)[viterbi_path[-1]]))\n",
    "        # Reverse the backward path.\n",
    "        viterbi_path.reverse()\n",
    "\n",
    "        if has_start_end_restrictions:\n",
    "            viterbi_path = viterbi_path[1:-1]\n",
    "\n",
    "        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.\n",
    "        viterbi_path = [j % num_tags for j in viterbi_path]\n",
    "        viterbi_paths.append(viterbi_path)\n",
    "\n",
    "    if flatten_output:\n",
    "        return viterbi_paths[0], viterbi_scores[0]\n",
    "\n",
    "    return viterbi_paths, viterbi_scores, all_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6ce415",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m100\u001b[39m, dimen])\n\u001b[0;32m      3\u001b[0m all_prob_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m x, all_prob \u001b[38;5;241m=\u001b[39m \u001b[43mviterbi_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43m_constraint_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constraint_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_transitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mstart_transitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_transitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mend_transitions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_transitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdimen\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39mcat(all_prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[25], line 68\u001b[0m, in \u001b[0;36mviterbi_tags\u001b[1;34m(logits, mask, top_k, _constraint_mask, _transitions, start_transitions, end_transitions)\u001b[0m\n\u001b[0;32m     64\u001b[0m         tag_sequence[sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, end_tag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#         print(torch.argmax(tag_sequence, dim=-1))\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# We pass the tags and the transitions to `viterbi_decode`.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m         viterbi_paths, viterbi_scores, all_prob \u001b[38;5;241m=\u001b[39m \u001b[43mviterbi_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtag_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransition_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m         stacked_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(all_prob))[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m     74\u001b[0m         all_prob_val\u001b[38;5;241m.\u001b[39mappend(stacked_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[26], line 101\u001b[0m, in \u001b[0;36mviterbi_decode\u001b[1;34m(tag_sequence, transition_matrix, tag_observations, allowed_start_transitions, allowed_end_transitions, top_k)\u001b[0m\n\u001b[0;32m     99\u001b[0m         scores, paths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(summed_potentials, k\u001b[38;5;241m=\u001b[39mmax_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#         print(scores, paths)\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m         \u001b[43mall_score\u001b[49m\u001b[38;5;241m.\u001b[39mappend(summed_potentials)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m#         print(\"best paths\", paths)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;66;03m# If we have an observation for this timestep, use it\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;66;03m# instead of the distribution over tags.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m         observation \u001b[38;5;241m=\u001b[39m tag_observations[timestep]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_score' is not defined"
     ]
    }
   ],
   "source": [
    "dimen = 73\n",
    "z = torch.randn([2, 100, dimen])\n",
    "all_prob_val = []\n",
    "x, all_prob = viterbi_tags(logits=z, top_k=1,\n",
    "             _constraint_mask= model.crf._constraint_mask[:dimen+2, :dimen+2], _transitions=model.crf.transitions[:dimen, :dimen],\n",
    "             start_transitions=model.crf.start_transitions[:dimen],\n",
    "            end_transitions = model.crf.end_transitions[:dimen])\n",
    "print(torch.argmax(torch.cat(all_prob, dim=0), dim=-1))\n",
    "print(x[0])\n",
    "# for i in torch.tensor_split(z, len(z), dim=0):\n",
    "#     all_score = []\n",
    "\n",
    "#     final_val = []\n",
    "#     x, all_prob = viterbi_tags(logits=i, top_k=1,\n",
    "#                  _constraint_mask= model.crf._constraint_mask[:dimen+2, :dimen+2], _transitions=model.crf.transitions[:dimen, :dimen],\n",
    "#                  start_transitions=model.crf.start_transitions[:dimen],\n",
    "#                 end_transitions = model.crf.end_transitions[:dimen])\n",
    "# #     print(x)\n",
    "#     stacked_t = torch.stack(list(reversed(all_prob))[1:])\n",
    "#     all_prob_val.append(stacked_t.unsqueeze(0))\n",
    "#     print(torch.argmax(stacked_t, dim=1))\n",
    "#     print(all(torch.tensor(x[0][0][0]) == torch.argmax(stacked_t, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008a99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77400101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(torch.stack(list(reversed(all_prob))), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85130b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.topk(all_score[-2], 1, 0))\n",
    "print(all_score[-1])\n",
    "all_score[-1][:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn([1,10]).unsqueeze(2)\n",
    "q = torch.randn([10,10])\n",
    "# print(p,q)\n",
    "model.crf._constraint_mask.shape\n",
    "model.crf.transitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _viterbi_decoding(emissions, transitions):\n",
    "    scores = torch.zeros(emissions.size(1))\n",
    "#     print(scores)\n",
    "#     back_pointers = torch.zeros(emissions.size()).int()\n",
    "#     scores = scores + emissions[0]\n",
    "#     # Generate most likely scores and paths for each step in sequence\n",
    "#     for i in range(1, emissions.size(0)):\n",
    "#         scores_with_transitions = scores.unsqueeze(1).expand_as(transitions) + transitions\n",
    "#         max_scores, back_pointers[i] = torch.max(scores_with_transitions, 0)\n",
    "#         scores = emissions[i] + max_scores\n",
    "#     # Generate the most likely path\n",
    "#     viterbi = [scores.numpy().argmax()]\n",
    "#     back_pointers = back_pointers.numpy()\n",
    "#     for bp in reversed(back_pointers[1:]):\n",
    "#         viterbi.append(bp[viterbi[-1]])\n",
    "#     viterbi.reverse()\n",
    "#     viterbi_score = scores.numpy().max()\n",
    "#     return viterbi_score, viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc242ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_viterbi_decoding(z, model.crf.transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8193e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c485c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk((p+q).view(-1,10), k=1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.crf._constraint_mask[:73, :73] *  model.crf.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c57994",
   "metadata": {},
   "outputs": [],
   "source": [
    "p+q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
