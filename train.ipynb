{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe9d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from dataloader import CoNLLReader\n",
    "from tqdm import tqdm\n",
    "from tutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8961a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del globals()['NERmodelbase']\n",
    "except:\n",
    "    pass\n",
    "from NERmodel import NERmodelbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be7f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bc8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    batch_ = list(zip(*batch))\n",
    "    tokens, masks, token_masks, gold_spans, tags, lstm_encoded = batch_[0], batch_[1], batch_[2], batch_[3], \\\n",
    "                                                                 batch_[4], batch_[5]\n",
    "    # print(tags)\n",
    "    max_len = np.max([len(token) for token in tokens])\n",
    "    # print(np.max([len(token) for token in tokens]), max_len)\n",
    "    token_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(tokenizer.pad_token_id)\n",
    "    tag_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(mconern['O'])\n",
    "    mask_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n",
    "    token_masks_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n",
    "    lstm_encoded_tensor = torch.zeros(size=(len(tokens), max_len, 256), dtype=torch.float)\n",
    "    # print(lstm_encoded.shape)\n",
    "    for i in range(len(tokens)):\n",
    "        tokens_ = tokens[i]\n",
    "        seq_len = len(tokens_)\n",
    "\n",
    "        token_tensor[i, :seq_len] = tokens_\n",
    "        tag_tensor[i, :seq_len] = tags[i]\n",
    "        mask_tensor[i, :seq_len] = masks[i]\n",
    "        token_masks_tensor[i, :seq_len] = token_masks[i]\n",
    "        lstm_encoded_tensor[i, 1:seq_len - 1, :] = lstm_encoded[i]\n",
    "\n",
    "    return token_tensor, tag_tensor, mask_tensor, token_masks_tensor, gold_spans, lstm_encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0fdbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, opt=False):\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-4, weight_decay=0.03)\n",
    "    if opt:\n",
    "        scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEP)\n",
    "        return [optimizer], [scheduler]\n",
    "    return [optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7f43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 1\n",
    "BATCH_SIZE = 64\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52888663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnut_iob = {'B-CORP': 0, 'I-CORP': 1, 'B-CW': 2, 'I-CW': 3, 'B-GRP': 4, 'I-GRP': 5, 'B-LOC': 6, 'I-LOC': 7,\n",
    "#             'B-PER': 8, 'I-PER': 9, 'B-PROD': 10, 'I-PROD': 11, 'O': 12}\n",
    "mconern = indvidual(mconer_grouped, True)\n",
    "reveremap = invert(mconer_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6f48032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if os.path.exists('train_load.pkl'):\n",
    "    with open('train_load.pkl', 'rb') as f:\n",
    "        ds = pickle.load(f)\n",
    "else:\n",
    "    print(\"reading from disk\")\n",
    "    ds = CoNLLReader(target_vocab=mconern, encoder_model=encoder_model, reversemap=reveremap, finegrained=fine)\n",
    "    ds.read_data(data=r'C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-train.conll')\n",
    "    with open('train_load.pkl', 'wb') as f:\n",
    "        pickle.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "268eafaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Reading file C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rah12937\\PycharmProjects\\mconer\\dataloader.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = self.lstm(torch.tensor(encoded))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 871 instances from file C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-dev.conll\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('valid_load.pkl'):\n",
    "    with open('valid_load.pkl', 'rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "else:\n",
    "    valid = CoNLLReader(target_vocab=mconern, encoder_model=encoder_model, reversemap=reveremap, finegrained=True)\n",
    "    valid.read_data(data=r'C:\\Users\\Rah12937\\PycharmProjects\\mconer\\multiconer2023\\train_dev\\en-dev.conll')\n",
    "    with open('valid_load.pkl', 'wb') as f:\n",
    "        pickle.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0a954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Rah12937\\PycharmProjects\\mconer\\NERmodel.py:26: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.w_omega, -0.1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "model = NERmodelbase(tag_to_id=mconern, device=device, encoder_model=encoder_model, dropout=0.3, use_lstm=True).to(\n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a362ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_batch, num_workers=0, shuffle=False)\n",
    "validloader = DataLoader(valid, batch_size=BATCH_SIZE, collate_fn=collate_batch, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14338915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of warm up step is 26\n"
     ]
    }
   ],
   "source": [
    "WARMUP_STEP = int(len(trainloader) * NUM_EPOCH * 0.1)\n",
    "print(f\"Number of warm up step is {WARMUP_STEP}\")\n",
    "optim, scheduler = get_optimizer(model, True, warmup=WARMUP_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02820976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "run_id = random.randint(1, 10000)\n",
    "run_name = f\"runid_{run_id}_EP_{NUM_EPOCH}_fine_xlm-b-birnnn-focal-loss-0_8-sep_lr-alpha-2-gama-4\"\n",
    "writer = SummaryWriter(run_name)\n",
    "step = 0\n",
    "running_loss = 0\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=run_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f140216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  37%|███████████████████████████████████████████████████████████                                                                                                     | 97/263 [01:04<01:50,  1.51batch/s]\n",
      "  0%|                                                                                                                                                                                   | 0/14 [00:00<?, ?batch/s]\u001b[A\n",
      "  7%|████████████▏                                                                                                                                                              | 1/14 [00:00<00:07,  1.75batch/s]\u001b[A\n",
      " 14%|████████████████████████▍                                                                                                                                                  | 2/14 [00:00<00:05,  2.30batch/s]\u001b[A\n",
      " 21%|████████████████████████████████████▋                                                                                                                                      | 3/14 [00:01<00:04,  2.49batch/s]\u001b[A\n",
      " 29%|████████████████████████████████████████████████▊                                                                                                                          | 4/14 [00:01<00:03,  2.51batch/s]\u001b[A\n",
      " 36%|█████████████████████████████████████████████████████████████                                                                                                              | 5/14 [00:02<00:03,  2.56batch/s]\u001b[A\n",
      " 43%|█████████████████████████████████████████████████████████████████████████▎                                                                                                 | 6/14 [00:02<00:03,  2.60batch/s]\u001b[A\n",
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 7/14 [00:02<00:02,  2.69batch/s]\u001b[A\n",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 8/14 [00:03<00:02,  2.69batch/s]\u001b[A\n",
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 9/14 [00:03<00:01,  2.70batch/s]\u001b[A\n",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 10/14 [00:03<00:01,  2.80batch/s]\u001b[A\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 11/14 [00:04<00:01,  2.83batch/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 12/14 [00:04<00:00,  2.79batch/s]\u001b[A\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 13/14 [00:04<00:00,  2.73batch/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.72batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1.135500).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 197/263 [02:19<00:46,  1.43batch/s]\n",
      "  0%|                                                                                                                                                                                   | 0/14 [00:00<?, ?batch/s]\u001b[A\n",
      "  7%|████████████▏                                                                                                                                                              | 1/14 [00:00<00:06,  1.90batch/s]\u001b[A\n",
      " 14%|████████████████████████▍                                                                                                                                                  | 2/14 [00:00<00:05,  2.25batch/s]\u001b[A\n",
      " 21%|████████████████████████████████████▋                                                                                                                                      | 3/14 [00:01<00:04,  2.50batch/s]\u001b[A\n",
      " 29%|████████████████████████████████████████████████▊                                                                                                                          | 4/14 [00:01<00:03,  2.53batch/s]\u001b[A\n",
      " 36%|█████████████████████████████████████████████████████████████                                                                                                              | 5/14 [00:02<00:03,  2.53batch/s]\u001b[A\n",
      " 43%|█████████████████████████████████████████████████████████████████████████▎                                                                                                 | 6/14 [00:02<00:03,  2.58batch/s]\u001b[A\n",
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 7/14 [00:02<00:02,  2.64batch/s]\u001b[A\n",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 8/14 [00:03<00:02,  2.59batch/s]\u001b[A\n",
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 9/14 [00:03<00:01,  2.64batch/s]\u001b[A\n",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 10/14 [00:03<00:01,  2.70batch/s]\u001b[A\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 11/14 [00:04<00:01,  2.73batch/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 12/14 [00:04<00:00,  2.63batch/s]\u001b[A\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 13/14 [00:05<00:00,  2.58batch/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.62batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.135500 --> 1.099600).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 263/263 [03:14<00:00,  1.35batch/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eval_step = 100\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    val_track = []\n",
    "    with tqdm(trainloader, unit='batch') as tepoch:\n",
    "        # model.train()\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        for i, data in enumerate(tepoch):\n",
    "            optim[0].zero_grad()\n",
    "            outputs, focal_loss = model(data)\n",
    "            loss = 0.2 * outputs['loss'] + 0.8 * focal_loss\n",
    "            running_loss += loss\n",
    "            loss.backward()\n",
    "            optim[0].step()\n",
    "            scheduler[0].step()\n",
    "            # if i % 10 == 0:  # print every 2000 mini-batches\n",
    "            model.spanf1.reset()\n",
    "            # writer.add_scalar('lr', scheduler[0].get_last_lr()[0], step)\n",
    "            # run validation\n",
    "            step += 1\n",
    "            if (step + 1) % eval_step == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    with tqdm(validloader, unit='batch') as tepoch:\n",
    "                        val_loss = 0\n",
    "                        for i, data in enumerate(tepoch):\n",
    "                            outputs, focal_loss = model(data, mode='predict')\n",
    "                            val_loss += 0.2 * outputs['loss'] + 0.8 * focal_loss\n",
    "                model.train()\n",
    "                writer.add_scalars(\"Loss\",\n",
    "                                   {\n",
    "                                       \"Train Loss\": round(running_loss.detach().cpu().numpy().ravel()[0] / 20, 4),\n",
    "                                       \"Valid Loss\": round(val_loss.detach().cpu().numpy().ravel()[0] / 20, 4),\n",
    "                                   }\n",
    "                                   , step)\n",
    "                writer.add_scalars(\"Metrics\", outputs['results'], step)\n",
    "                # print(outputs['results'])\n",
    "                # writer.add_scalar(\"Loss/Test\", round(val_loss.numpy()[0] / len(validloader), 4), step)\n",
    "                # writer.add_scalar(\"Loss/Valid\",\n",
    "                #                   round(val_loss.detach().cpu().numpy().ravel()[0] / 20, 4), step)\n",
    "                val_track.append(round(val_loss.detach().cpu().numpy().ravel()[0] / eval_step, 4))\n",
    "                running_loss = 0\n",
    "\n",
    "                early_stopping(round(val_loss.detach().cpu().numpy().ravel()[0] / eval_step, 4), model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Stopping early\")\n",
    "                    break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e250e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(next(iter(trainloader))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3f943b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbcb88c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 73)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf.transitions.cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51a3cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([22, 30, 2, 16, 12, 14, 64, 22, 44, 22], 19.867223739624023),\n",
       " ([50, 52, 8, 2, 24, 44, 45, 28, 64, 30], 22.552013397216797)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf.viterbi_tags(torch.randn([2, 10, 73]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8dc6ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn([1, 5, 73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2287e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp.nn.util as util\n",
    "def viterbi_tags(\n",
    "    logits: torch.Tensor, mask: torch.BoolTensor = None, top_k: int = None,\n",
    "    _constraint_mask= None, _transitions=None, start_transitions=None, end_transitions=None\n",
    "):\n",
    "    if mask is None:\n",
    "        mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)\n",
    "\n",
    "    if top_k is None:\n",
    "        top_k = 1\n",
    "        flatten_output = True\n",
    "    else:\n",
    "        flatten_output = False\n",
    "\n",
    "    _, max_seq_length, num_tags = logits.size()\n",
    "#     print(num_tags)\n",
    "    # Get the tensors out of the variables\n",
    "    logits, mask = logits.data, mask.data\n",
    "\n",
    "    # Augment transitions matrix with start and end transitions\n",
    "    start_tag = num_tags\n",
    "    end_tag = num_tags + 1\n",
    "    transitions = torch.full((num_tags + 2, num_tags + 2), -10000.0, device=logits.device)\n",
    "\n",
    "    # Apply transition constraints\n",
    "    constrained_transitions = _transitions * _constraint_mask[:num_tags, :num_tags] + -10000.0 * (1 - _constraint_mask[:num_tags, :num_tags])\n",
    "    transitions[:num_tags, :num_tags] = constrained_transitions.data\n",
    "\n",
    "    if True:\n",
    "        transitions[\n",
    "            start_tag, :num_tags\n",
    "        ] = start_transitions.detach() * _constraint_mask[\n",
    "            start_tag, :num_tags\n",
    "        ].data + -10000.0 * (\n",
    "            1 - _constraint_mask[start_tag, :num_tags].detach()\n",
    "        )\n",
    "        transitions[:num_tags, end_tag] = end_transitions.detach() * _constraint_mask[\n",
    "            :num_tags, end_tag\n",
    "        ].data + -10000.0 * (1 - _constraint_mask[:num_tags, end_tag].detach())\n",
    "    else:\n",
    "        transitions[start_tag, :num_tags] = -10000.0 * (\n",
    "            1 - self._constraint_mask[start_tag, :num_tags].detach()\n",
    "        )\n",
    "        transitions[:num_tags, end_tag] = -10000.0 * (\n",
    "            1 - self._constraint_mask[:num_tags, end_tag].detach()\n",
    "        )\n",
    "\n",
    "    best_paths = []\n",
    "    # Pad the max sequence length by 2 to account for start_tag + end_tag.\n",
    "    tag_sequence = torch.empty(max_seq_length + 2, num_tags + 2, device=logits.device)\n",
    "\n",
    "    for prediction, prediction_mask in zip(logits, mask):\n",
    "        mask_indices = prediction_mask.nonzero(as_tuple=False).squeeze()\n",
    "        masked_prediction = torch.index_select(prediction, 0, mask_indices)\n",
    "        sequence_length = masked_prediction.shape[0]\n",
    "\n",
    "        # Start with everything totally unlikely\n",
    "        tag_sequence.fill_(-10000.0)\n",
    "        # At timestep 0 we must have the START_TAG\n",
    "        tag_sequence[0, start_tag] = 0.0\n",
    "        # At steps 1, ..., sequence_length we just use the incoming prediction\n",
    "        tag_sequence[1 : (sequence_length + 1), :num_tags] = masked_prediction\n",
    "        # And at the last timestep we must have the END_TAG\n",
    "        tag_sequence[sequence_length + 1, end_tag] = 0.0\n",
    "#         print(torch.argmax(tag_sequence, dim=-1))\n",
    "\n",
    "        # We pass the tags and the transitions to `viterbi_decode`.\n",
    "        viterbi_paths, viterbi_scores = viterbi_decode(\n",
    "            tag_sequence=tag_sequence[: (sequence_length + 2)],\n",
    "            transition_matrix=transitions,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "#         print(viterbi_paths, viterbi_scores)\n",
    "        top_k_paths = []\n",
    "        for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):\n",
    "            # Get rid of START and END sentinels and append.\n",
    "            viterbi_path = viterbi_path[1:-1]\n",
    "            top_k_paths.append((viterbi_path, viterbi_score.item()))\n",
    "        best_paths.append(top_k_paths)\n",
    "\n",
    "    if flatten_output:\n",
    "        return [top_k_paths[0] for top_k_paths in best_paths]\n",
    "\n",
    "    return best_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "93da05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "def viterbi_decode(\n",
    "    tag_sequence: torch.Tensor,\n",
    "    transition_matrix: torch.Tensor,\n",
    "    tag_observations: Optional[List[int]] = None,\n",
    "    allowed_start_transitions: torch.Tensor = None,\n",
    "    allowed_end_transitions: torch.Tensor = None,\n",
    "    top_k: int = None,\n",
    "):\n",
    "    print(tag_sequence.shape)\n",
    "    if top_k is None:\n",
    "        top_k = 1\n",
    "        flatten_output = True\n",
    "    elif top_k >= 1:\n",
    "        flatten_output = False\n",
    "    else:\n",
    "        raise ValueError(f\"top_k must be either None or an integer >=1. Instead received {top_k}\")\n",
    "\n",
    "    sequence_length, num_tags = list(tag_sequence.size())\n",
    "\n",
    "    has_start_end_restrictions = (\n",
    "        allowed_end_transitions is not None or allowed_start_transitions is not None\n",
    "    )\n",
    "\n",
    "    if has_start_end_restrictions:\n",
    "\n",
    "        if allowed_end_transitions is None:\n",
    "            allowed_end_transitions = torch.zeros(num_tags)\n",
    "        if allowed_start_transitions is None:\n",
    "            allowed_start_transitions = torch.zeros(num_tags)\n",
    "\n",
    "        num_tags = num_tags + 2\n",
    "        new_transition_matrix = torch.zeros(num_tags, num_tags)\n",
    "        new_transition_matrix[:-2, :-2] = transition_matrix\n",
    "\n",
    "        # Start and end transitions are fully defined, but cannot transition between each other.\n",
    "\n",
    "        allowed_start_transitions = torch.cat(\n",
    "            [allowed_start_transitions, torch.tensor([-math.inf, -math.inf])]\n",
    "        )\n",
    "        allowed_end_transitions = torch.cat(\n",
    "            [allowed_end_transitions, torch.tensor([-math.inf, -math.inf])]\n",
    "        )\n",
    "\n",
    "        # First define how we may transition FROM the start and end tags.\n",
    "        new_transition_matrix[-2, :] = allowed_start_transitions\n",
    "        # We cannot transition from the end tag to any tag.\n",
    "        new_transition_matrix[-1, :] = -math.inf\n",
    "\n",
    "        new_transition_matrix[:, -1] = allowed_end_transitions\n",
    "        # We cannot transition to the start tag from any tag.\n",
    "        new_transition_matrix[:, -2] = -math.inf\n",
    "\n",
    "        transition_matrix = new_transition_matrix\n",
    "\n",
    "    if tag_observations:\n",
    "        if len(tag_observations) != sequence_length:\n",
    "            raise ConfigurationError(\n",
    "                \"Observations were provided, but they were not the same length \"\n",
    "                \"as the sequence. Found sequence of length: {} and evidence: {}\".format(\n",
    "                    sequence_length, tag_observations\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        tag_observations = [-1 for _ in range(sequence_length)]\n",
    "\n",
    "    if has_start_end_restrictions:\n",
    "        tag_observations = [num_tags - 2] + tag_observations + [num_tags - 1]\n",
    "        zero_sentinel = torch.zeros(1, num_tags)\n",
    "        extra_tags_sentinel = torch.ones(sequence_length, 2) * -math.inf\n",
    "        tag_sequence = torch.cat([tag_sequence, extra_tags_sentinel], -1)\n",
    "        tag_sequence = torch.cat([zero_sentinel, tag_sequence, zero_sentinel], 0)\n",
    "        sequence_length = tag_sequence.size(0)\n",
    "\n",
    "    path_scores = []\n",
    "    path_indices = []\n",
    "\n",
    "    if tag_observations[0] != -1:\n",
    "        one_hot = torch.zeros(num_tags)\n",
    "        one_hot[tag_observations[0]] = 100000.0\n",
    "        path_scores.append(one_hot.unsqueeze(0))\n",
    "    else:\n",
    "        path_scores.append(tag_sequence[0, :].unsqueeze(0))\n",
    "\n",
    "    # Evaluate the scores for all possible paths.\n",
    "#     print(\"length\", path_scores[0].shape)\n",
    "    all_score = []\n",
    "    for timestep in range(1, sequence_length):\n",
    "#         print(timestep, path_scores[timestep - 1].shape, transition_matrix.shape)\n",
    "        # Add pairwise potentials to current scores.\n",
    "        summed_potentials = path_scores[timestep - 1].unsqueeze(2) + transition_matrix\n",
    "        summed_potentials = summed_potentials.view(-1, num_tags)\n",
    "#         print(summed_potentials.shape, num_tags)\n",
    "        # Best pairwise potential path score from the previous timestep.\n",
    "        max_k = min(summed_potentials.size()[0], top_k)\n",
    "        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)\n",
    "        all_score.append(scores)\n",
    "#         print(\"best paths\", paths)\n",
    "        # If we have an observation for this timestep, use it\n",
    "        # instead of the distribution over tags.\n",
    "        observation = tag_observations[timestep]\n",
    "        # Warn the user if they have passed\n",
    "        # invalid/extremely unlikely evidence.\n",
    "        if tag_observations[timestep - 1] != -1 and observation != -1:\n",
    "            if transition_matrix[tag_observations[timestep - 1], observation] < -10000:\n",
    "                logger.warning(\n",
    "                    \"The pairwise potential between tags you have passed as \"\n",
    "                    \"observations is extremely unlikely. Double check your evidence \"\n",
    "                    \"or transition potentials!\"\n",
    "                )\n",
    "        if observation != -1:\n",
    "            one_hot = torch.zeros(num_tags)\n",
    "            one_hot[observation] = 100000.0\n",
    "            path_scores.append(one_hot.unsqueeze(0))\n",
    "        else:\n",
    "            path_scores.append(tag_sequence[timestep, :] + scores)\n",
    "        path_indices.append(paths.squeeze())\n",
    "#     print(\"Simple Argmax\", torch.argmax(torch.cat(path_scores[1:-1], dim=0), dim=-1))\n",
    "    print(torch.argmax(torch.cat(all_score, dim=0), dim=-1))\n",
    "    # Construct the most likely sequence backwards.\n",
    "    path_scores_v = path_scores[-1].view(-1)\n",
    "#     print(path_scores[-1].shape)\n",
    "    max_k = min(path_scores_v.size()[0], top_k)\n",
    "    viterbi_scores, best_paths = torch.topk(path_scores_v, k=max_k, dim=0)\n",
    "#     print(viterbi_scores, best_paths, max_k)\n",
    "    viterbi_paths = []\n",
    "    for i in range(max_k):\n",
    "        viterbi_path = [best_paths[i]]\n",
    "        for backward_timestep in reversed(path_indices):\n",
    "            print(backward_timestep, viterbi_path[-1])\n",
    "            viterbi_path.append(int(backward_timestep.view(-1)[viterbi_path[-1]]))\n",
    "        # Reverse the backward path.\n",
    "        viterbi_path.reverse()\n",
    "\n",
    "        if has_start_end_restrictions:\n",
    "            viterbi_path = viterbi_path[1:-1]\n",
    "\n",
    "        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.\n",
    "        viterbi_path = [j % num_tags for j in viterbi_path]\n",
    "        viterbi_paths.append(viterbi_path)\n",
    "\n",
    "    if flatten_output:\n",
    "        return viterbi_paths[0], viterbi_scores[0]\n",
    "\n",
    "    return viterbi_paths, viterbi_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8e6ce415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 75])\n",
      "tensor([52, 28, 74, 74, 74, 74])\n",
      "tensor([30,  0, 30,  2, 30,  5, 30,  6, 30,  8, 30, 10, 30, 13, 30, 14, 30, 17,\n",
      "        30, 18, 30, 21, 30, 22, 30, 24, 30, 26, 30, 28, 30, 30, 30, 32, 30, 34,\n",
      "        30, 36, 30, 38, 30, 40, 30, 42, 30, 44, 30, 46, 30, 48, 30, 50, 30, 52,\n",
      "        30, 54, 30, 57, 30, 58, 30, 60, 30, 62, 30, 64, 30, 66, 30, 68, 30, 71,\n",
      "        30, 30, 57]) tensor(74)\n",
      "tensor([71,  0, 71,  3, 10,  4, 71,  6, 10,  8, 10, 10, 10, 12, 71, 14, 10, 16,\n",
      "        10, 19, 71, 20, 71, 22, 10, 24, 10, 26, 10, 28, 71, 30, 10, 32, 71, 34,\n",
      "        10, 37, 71, 39, 71, 40, 10, 42, 10, 45, 10, 46, 10, 48, 71, 51, 10, 52,\n",
      "        10, 54, 71, 56, 71, 58, 71, 60, 10, 62, 10, 64, 10, 66, 71, 68, 71, 71,\n",
      "        71, 10, 45]) 57\n",
      "tensor([50,  0, 50,  2, 70,  4, 70,  6, 70,  8, 70, 10, 70, 12, 50, 15, 50, 16,\n",
      "        70, 18, 50, 20, 50, 22, 70, 24, 70, 26, 70, 28, 70, 30, 70, 32, 50, 34,\n",
      "        50, 37, 50, 38, 70, 40, 50, 43, 50, 44, 70, 47, 50, 48, 50, 50, 70, 53,\n",
      "        50, 54, 70, 56, 70, 58, 50, 60, 70, 62, 70, 65, 70, 66, 70, 68, 50, 70,\n",
      "        70, 50, 70]) 56\n",
      "tensor([52,  0, 52,  2, 52,  4, 52,  6, 52,  8, 62, 10, 52, 12, 52, 14, 52, 16,\n",
      "        62, 18, 62, 20, 52, 22, 52, 24, 52, 26, 52, 28, 52, 30, 52, 32, 52, 34,\n",
      "        52, 36, 52, 39, 62, 40, 52, 42, 52, 44, 62, 46, 52, 48, 52, 50, 52, 52,\n",
      "        52, 54, 52, 56, 52, 58, 52, 60, 52, 62, 52, 65, 52, 66, 52, 68, 52, 70,\n",
      "        62, 52, 46]) 70\n",
      "tensor([38,  0, 38,  2, 38,  4, 38,  6, 38,  8, 38, 10, 38, 12, 38, 14, 38, 16,\n",
      "        38, 18, 38, 20, 38, 22, 38, 24, 38, 26, 38, 28, 38, 30, 38, 32, 38, 34,\n",
      "        38, 36, 38, 38, 38, 40, 38, 42, 38, 44, 38, 46, 38, 48, 38, 50, 38, 52,\n",
      "        38, 54, 38, 56, 38, 58, 38, 60, 38, 62, 38, 64, 38, 66, 38, 68, 38, 70,\n",
      "        38, 38, 16]) 52\n",
      "tensor([73,  1, 73,  2, 73,  4, 73, 73, 73,  8, 73, 73, 73, 12, 73, 14, 73, 16,\n",
      "        73, 19, 73, 21, 73, 23, 73, 24, 73, 73, 73, 73, 73, 31, 73, 33, 73, 73,\n",
      "        73, 37, 73, 39, 73, 40, 73, 42, 73, 44, 73, 47, 73, 48, 73, 51, 73, 73,\n",
      "        73, 55, 73, 73, 73, 59, 73, 60, 73, 62, 73, 73, 73, 73, 73, 68, 73, 70,\n",
      "        73, 73, 57]) 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[([38, 52, 70, 56, 57], 11.473505973815918)]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_tags(logits=z, top_k=1,\n",
    "             _constraint_mask= model.crf._constraint_mask, _transitions=model.crf.transitions,\n",
    "             start_transitions=model.crf.start_transitions,\n",
    "            end_transitions = model.crf.end_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f67992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn([1,10]).unsqueeze(2)\n",
    "q = torch.randn([10,10])\n",
    "# print(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9e7c5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _viterbi_decoding(emissions, transitions):\n",
    "    scores = torch.zeros(emissions.size(1))\n",
    "#     print(scores)\n",
    "#     back_pointers = torch.zeros(emissions.size()).int()\n",
    "#     scores = scores + emissions[0]\n",
    "#     # Generate most likely scores and paths for each step in sequence\n",
    "#     for i in range(1, emissions.size(0)):\n",
    "#         scores_with_transitions = scores.unsqueeze(1).expand_as(transitions) + transitions\n",
    "#         max_scores, back_pointers[i] = torch.max(scores_with_transitions, 0)\n",
    "#         scores = emissions[i] + max_scores\n",
    "#     # Generate the most likely path\n",
    "#     viterbi = [scores.numpy().argmax()]\n",
    "#     back_pointers = back_pointers.numpy()\n",
    "#     for bp in reversed(back_pointers[1:]):\n",
    "#         viterbi.append(bp[viterbi[-1]])\n",
    "#     viterbi.reverse()\n",
    "#     viterbi_score = scores.numpy().max()\n",
    "#     return viterbi_score, viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fc242ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "_viterbi_decoding(z, model.crf.transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a8193e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1792e+00, -2.7135e-02, -7.7828e-01,  1.0579e+00, -1.4069e+00,\n",
       "          2.9101e-01,  1.3014e+00, -9.4263e-01, -7.3761e-01, -3.2752e-01,\n",
       "          1.3143e+00, -9.0450e-01,  2.5484e-01, -1.7810e+00, -3.5424e-01,\n",
       "          8.2577e-01,  1.2528e+00,  2.9811e-01, -8.1395e-01, -1.8555e+00,\n",
       "         -8.4935e-01, -1.9918e+00,  6.0543e-02, -3.1424e-01, -3.9017e-01,\n",
       "          2.4125e-01,  9.8717e-01,  1.6513e-01, -6.9897e-01, -1.1438e+00,\n",
       "          5.5704e-01,  7.1476e-01,  1.2251e+00,  9.0660e-01,  4.3582e-02,\n",
       "         -1.4489e+00, -1.5145e+00,  7.7617e-01,  2.4574e+00,  6.4148e-01,\n",
       "          8.6552e-02, -6.3901e-01,  1.8131e+00, -1.7130e+00,  5.1753e-01,\n",
       "          3.5789e-01, -1.4740e-01,  5.9639e-01,  9.0849e-01, -1.5356e+00,\n",
       "         -1.1607e+00,  4.4085e-01, -1.9936e+00, -9.8936e-01, -3.0054e-01,\n",
       "         -1.1908e+00, -4.1412e-01,  8.7369e-01,  6.1928e-01,  3.9566e-01,\n",
       "         -1.5622e+00,  6.1427e-01, -3.1695e-01, -2.2818e+00,  8.9419e-01,\n",
       "          4.0324e-01, -1.7241e+00, -6.1458e-01, -1.4383e+00, -4.9064e-01,\n",
       "         -7.6685e-01,  6.2275e-01, -1.0746e-01],\n",
       "        [-9.1765e-01, -1.0075e+00, -3.9610e-01, -1.2790e+00, -2.7192e-03,\n",
       "          2.8480e-01, -1.5191e+00,  8.3505e-01, -8.9134e-01, -5.0877e-02,\n",
       "          4.1780e-01, -2.8681e-01,  6.3711e-01,  6.0492e-01,  4.7547e-01,\n",
       "          1.0847e-01,  4.6735e-01,  1.0839e+00,  6.4252e-02, -2.2556e-01,\n",
       "          2.0991e-01, -1.4625e+00, -1.6460e+00, -1.0181e-02,  5.6480e-01,\n",
       "         -5.6193e-01,  1.9070e-01,  1.1196e-01, -1.3873e+00,  8.4410e-01,\n",
       "         -5.2058e-01, -8.3919e-02,  6.2634e-01, -9.3769e-01, -5.9253e-01,\n",
       "          8.2258e-01,  1.3317e+00,  3.3516e-01, -9.0393e-01, -5.5062e-01,\n",
       "          9.2845e-01, -3.3209e-01,  7.9014e-01,  1.7442e-02, -1.6761e-01,\n",
       "          8.0672e-01,  1.1150e+00,  4.2893e-01, -8.7826e-01, -5.1489e-01,\n",
       "          1.0809e+00, -1.1674e-01,  1.9176e+00, -3.0863e-01, -1.0754e+00,\n",
       "         -3.3092e-01,  4.8817e-01, -1.1149e+00, -5.6776e-01, -7.5811e-01,\n",
       "         -5.5726e-02,  1.0004e+00,  1.6437e+00,  3.4759e-01, -1.7058e+00,\n",
       "          1.9326e+00,  5.3729e-01, -8.4837e-01,  1.0912e+00,  4.4809e-02,\n",
       "          2.0227e-01,  1.8215e+00, -6.2732e-01],\n",
       "        [-8.4519e-01,  1.2238e+00, -8.4234e-02,  3.3082e-01, -2.5546e-01,\n",
       "         -1.3289e+00,  2.3300e-01, -7.2262e-01, -1.3709e+00,  7.3008e-01,\n",
       "         -4.5834e-02,  1.0422e+00,  4.9391e-01, -4.2077e-01, -1.2741e+00,\n",
       "          9.6264e-01,  4.8188e-02,  1.6747e-01,  4.6396e-01, -8.0305e-01,\n",
       "          2.7576e-01, -3.0575e-01, -1.2084e+00, -1.2358e+00,  1.0918e+00,\n",
       "          3.8757e-01, -2.7103e-01,  6.4512e-01,  2.3919e-01,  2.4810e-01,\n",
       "          8.5659e-01,  4.7177e-01,  1.0442e+00,  1.8862e-01,  8.4392e-02,\n",
       "          1.2686e+00,  9.2137e-01,  9.0824e-01,  4.0445e-01, -9.4899e-01,\n",
       "          1.1433e+00,  1.6765e+00, -6.7233e-01,  1.5173e+00,  1.0150e+00,\n",
       "          4.2812e-01, -7.1077e-01,  2.2834e-01,  2.2108e-01, -4.5590e-01,\n",
       "          2.2817e+00,  1.0285e+00, -7.1719e-01,  1.5171e+00, -8.0020e-01,\n",
       "         -1.3110e+00,  5.0982e-01, -1.2214e-01, -6.5682e-01, -1.1365e+00,\n",
       "         -4.2923e-01,  6.7615e-01,  7.2298e-01,  1.7331e-01, -2.2676e+00,\n",
       "          1.1102e+00,  1.2759e+00, -1.6389e+00,  2.0202e-02, -4.7002e-01,\n",
       "          2.1068e+00,  1.3333e+00, -1.5455e+00],\n",
       "        [ 1.9772e-01,  9.7441e-02, -1.2201e+00,  2.0674e+00,  8.5690e-01,\n",
       "         -2.1242e-02,  7.0778e-01, -6.7219e-01, -2.0403e+00, -3.1448e-01,\n",
       "          1.5643e+00,  1.1206e+00,  4.4955e-01,  7.7487e-01, -7.6119e-02,\n",
       "          8.9046e-01,  1.7641e-01,  5.9543e-01, -5.4765e-01,  1.2411e+00,\n",
       "          9.6967e-01, -1.6715e+00, -3.2067e-01, -1.7261e+00,  1.9933e-01,\n",
       "         -5.1099e-01, -4.0901e-01,  7.5844e-01, -2.1118e-01, -2.5730e-01,\n",
       "          2.4066e-01, -3.2456e-01,  3.8052e-02,  4.9774e-01, -1.2213e+00,\n",
       "         -6.2956e-01, -4.7085e-01,  2.1789e+00, -2.3690e+00,  1.3107e-01,\n",
       "          1.3019e+00, -1.4461e+00, -9.5217e-01,  2.4100e-01, -2.0451e-02,\n",
       "          1.8293e+00, -1.7311e-01, -1.0905e+00,  1.7287e-01,  1.3639e+00,\n",
       "         -1.7898e+00, -9.1008e-02,  3.5230e-01,  1.2944e+00, -1.0974e-01,\n",
       "         -6.6289e-01,  1.9046e-01,  1.2535e-01,  1.9103e-01,  1.8006e-01,\n",
       "         -5.3255e-01, -8.0055e-02,  1.3007e+00, -3.5100e-01,  1.2050e+00,\n",
       "         -1.8642e-01, -4.0356e-02,  4.7344e-01, -8.1697e-01,  4.2582e-01,\n",
       "          1.2176e-01,  1.1564e+00, -5.0276e-01],\n",
       "        [-1.4855e+00, -9.7371e-01,  8.2976e-01, -2.4103e+00, -4.9987e-01,\n",
       "          1.1057e+00, -1.0913e-01, -1.0162e-01,  8.0371e-01, -5.9941e-01,\n",
       "          1.0136e+00,  9.4806e-01, -1.4729e+00,  6.0525e-01,  1.5202e+00,\n",
       "         -1.8234e+00, -8.2189e-01,  3.7468e-01, -1.8998e-01,  1.0165e+00,\n",
       "         -2.6390e+00, -2.6876e-01, -9.0635e-01, -8.2186e-01,  1.3817e+00,\n",
       "          1.3487e+00, -1.0397e+00, -2.7388e+00, -1.7729e+00, -1.0852e-01,\n",
       "          1.6572e+00,  6.9710e-01, -5.7531e-02, -2.2216e-02, -1.1112e-01,\n",
       "          1.5743e+00,  9.3638e-01, -1.2736e+00, -2.2209e+00, -2.6510e-01,\n",
       "          1.5654e+00,  1.1075e+00,  1.3892e+00,  1.0031e+00,  1.5615e+00,\n",
       "         -6.4067e-01,  1.1972e+00,  6.3598e-01,  5.1449e-02,  1.4675e-01,\n",
       "          1.4660e+00,  5.6923e-01, -2.2295e-01, -2.5127e-02, -4.2092e-01,\n",
       "          9.5095e-02,  7.7158e-01,  2.1146e+00, -2.1395e-01, -4.7089e-01,\n",
       "          1.5321e-01,  1.4117e+00,  6.1083e-01,  6.2297e-01,  6.3357e-01,\n",
       "         -3.8354e-02,  4.6244e-01, -6.6751e-01, -4.2470e-02,  7.7410e-01,\n",
       "         -2.5167e-02,  4.7188e-02,  2.6252e-01]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c9c485c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[3.1213, 0.9428, 0.6200, 1.2594, 1.3068, 1.4101, 0.7416, 1.1139, 1.8678,\n",
       "         1.8825]]),\n",
       "indices=tensor([[6, 3, 5, 5, 0, 6, 3, 6, 6, 5]]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk((p+q).view(-1,10), k=1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f9fb509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1321,  0.1094, -0.1860,  ..., -0.2469,  0.0000, -0.0261],\n",
       "        [ 0.1491,  0.2218,  0.0244,  ..., -0.0113,  0.0000, -0.0211],\n",
       "        [ 0.0466,  0.0000,  0.0872,  ..., -0.2424, -0.0000, -0.3008],\n",
       "        ...,\n",
       "        [-0.1596, -0.0000, -0.1920,  ..., -0.2957,  0.2783, -0.0859],\n",
       "        [ 0.0113,  0.0000,  0.0119,  ..., -0.1425,  0.0664, -0.1141],\n",
       "        [-0.0490, -0.0000, -0.0082,  ..., -0.1509, -0.0000,  0.1871]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf._constraint_mask[:73, :73] *  model.crf.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c57994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
